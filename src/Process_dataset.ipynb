{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>barelytrues</th>\n",
       "      <th>falses</th>\n",
       "      <th>halftrues</th>\n",
       "      <th>mostlystrues</th>\n",
       "      <th>pantsonfires</th>\n",
       "      <th>context</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11972.json</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>Radio interview</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11685.json</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a news conference</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11096.json</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>military,veterans,voting-record</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>President-Elect</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>63</td>\n",
       "      <td>114</td>\n",
       "      <td>51</td>\n",
       "      <td>37</td>\n",
       "      <td>61</td>\n",
       "      <td>comments on ABC's This Week.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5209.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
       "      <td>medicare,message-machine-2012,campaign-adverti...</td>\n",
       "      <td>rob-cornilles</td>\n",
       "      <td>consultant</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>republican</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a radio show</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7070.json</td>\n",
       "      <td>TRUE</td>\n",
       "      <td>Says that Tennessee law requires that schools ...</td>\n",
       "      <td>county-budget,county-government,education,taxes</td>\n",
       "      <td>stand-children-tennessee</td>\n",
       "      <td>Child and education advocacy organization.</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>in a post on Facebook.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>7013.json</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>Says U.S. Rep. Charles Bass wants to privatize...</td>\n",
       "      <td>social-security</td>\n",
       "      <td>ann-mclane-kuster</td>\n",
       "      <td>Attorney</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>an ad, Ã¢â‚¬Å“Janice,Ã¢â‚¬Â released Septembe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>2661.json</td>\n",
       "      <td>pants-fire</td>\n",
       "      <td>In the past two years, Democrats have spent mo...</td>\n",
       "      <td>federal-budget,history</td>\n",
       "      <td>eric-cantor</td>\n",
       "      <td>House Majority Leader</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>republican</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>an interview on Comedy Central's Daily Show wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>3419.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>For the first time in more than a decade, impo...</td>\n",
       "      <td>energy,oil-spill,trade</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70</td>\n",
       "      <td>71</td>\n",
       "      <td>160</td>\n",
       "      <td>163</td>\n",
       "      <td>9</td>\n",
       "      <td>a press conference</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>12548.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Says Donald Trump has bankrupted his companies...</td>\n",
       "      <td>candidates-biography</td>\n",
       "      <td>hillary-clinton</td>\n",
       "      <td>Presidential candidate</td>\n",
       "      <td>New York</td>\n",
       "      <td>democrat</td>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>a speech on the economy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>9117.json</td>\n",
       "      <td>barely-true</td>\n",
       "      <td>No one claims the report vindicating New Jerse...</td>\n",
       "      <td>candidates-biography,infrastructure</td>\n",
       "      <td>rudy-giuliani</td>\n",
       "      <td>Attorney</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>comments on NBC's \"Meet the Press\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8468 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id        class  \\\n",
       "0     11972.json         TRUE   \n",
       "1     11685.json        FALSE   \n",
       "2     11096.json        FALSE   \n",
       "3      5209.json    half-true   \n",
       "4      7070.json         TRUE   \n",
       "...          ...          ...   \n",
       "8463   7013.json  barely-true   \n",
       "8464   2661.json   pants-fire   \n",
       "8465   3419.json    half-true   \n",
       "8466  12548.json  mostly-true   \n",
       "8467   9117.json  barely-true   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Building a wall on the U.S.-Mexico border will...   \n",
       "1     Wisconsin is on pace to double the number of l...   \n",
       "2     Says John McCain has done nothing to help the ...   \n",
       "3     Suzanne Bonamici supports a plan that will cut...   \n",
       "4     Says that Tennessee law requires that schools ...   \n",
       "...                                                 ...   \n",
       "8463  Says U.S. Rep. Charles Bass wants to privatize...   \n",
       "8464  In the past two years, Democrats have spent mo...   \n",
       "8465  For the first time in more than a decade, impo...   \n",
       "8466  Says Donald Trump has bankrupted his companies...   \n",
       "8467  No one claims the report vindicating New Jerse...   \n",
       "\n",
       "                                                  title  \\\n",
       "0                                           immigration   \n",
       "1                                                  jobs   \n",
       "2                       military,veterans,voting-record   \n",
       "3     medicare,message-machine-2012,campaign-adverti...   \n",
       "4       county-budget,county-government,education,taxes   \n",
       "...                                                 ...   \n",
       "8463                                    social-security   \n",
       "8464                             federal-budget,history   \n",
       "8465                             energy,oil-spill,trade   \n",
       "8466                               candidates-biography   \n",
       "8467                candidates-biography,infrastructure   \n",
       "\n",
       "                       speaker                                   job title  \\\n",
       "0                   rick-perry                                    Governor   \n",
       "1            katrina-shankland                        State representative   \n",
       "2                 donald-trump                             President-Elect   \n",
       "3                rob-cornilles                                  consultant   \n",
       "4     stand-children-tennessee  Child and education advocacy organization.   \n",
       "...                        ...                                         ...   \n",
       "8463         ann-mclane-kuster                                    Attorney   \n",
       "8464               eric-cantor                       House Majority Leader   \n",
       "8465              barack-obama                                   President   \n",
       "8466           hillary-clinton                      Presidential candidate   \n",
       "8467             rudy-giuliani                                    Attorney   \n",
       "\n",
       "              state       party  barelytrues  falses  halftrues  mostlystrues  \\\n",
       "0             Texas  republican           30      30         42            23   \n",
       "1         Wisconsin    democrat            2       1          0             0   \n",
       "2          New York  republican           63     114         51            37   \n",
       "3            Oregon  republican            1       1          3             1   \n",
       "4         Tennessee        none            0       0          0             0   \n",
       "...             ...         ...          ...     ...        ...           ...   \n",
       "8463  New Hampshire    democrat            2       1          3             0   \n",
       "8464       Virginia  republican            9       6          4             4   \n",
       "8465       Illinois    democrat           70      71        160           163   \n",
       "8466       New York    democrat           40      29         69            76   \n",
       "8467       New York  republican            9      11         10             7   \n",
       "\n",
       "      pantsonfires                                            context  flag  \n",
       "0               18                                    Radio interview     1  \n",
       "1                0                                  a news conference     0  \n",
       "2               61                       comments on ABC's This Week.     0  \n",
       "3                1                                       a radio show     0  \n",
       "4                0                             in a post on Facebook.     1  \n",
       "...            ...                                                ...   ...  \n",
       "8463             0  an ad, Ã¢â‚¬Å“Janice,Ã¢â‚¬Â released Septembe...     0  \n",
       "8464             4  an interview on Comedy Central's Daily Show wi...     0  \n",
       "8465             9                                 a press conference     0  \n",
       "8466             7                            a speech on the economy     1  \n",
       "8467             3                 comments on NBC's \"Meet the Press\"     0  \n",
       "\n",
       "[8468 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "    Script to process dataset, calculating TextBlob and VADER polarity and subjectivity scores\n",
    "\n",
    "    Assumes dataset features contains columns titled \"text\" and \"title\" \n",
    "\n",
    "    Assumes input and output files are CSV\n",
    "\n",
    "    Assumes that class indicator column is called 'label'\n",
    "    TODO: If this needs to be a program that the prof/TA can use, then CLI args should be used for file names\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "read_file = r\"../data/Des_fake_news/LIAR.csv\"\n",
    "read_sheet = \"valid\"\n",
    "write_file = r\"../data/Des_fake_news/LIAR_PROCESSED_2.csv\"\n",
    "\n",
    "#data = pd.read_excel(read_file, read_sheet)\n",
    "data = pd.read_csv(read_file)\n",
    "\n",
    "data = data.dropna(axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Natural language preprocessing\n",
    "\n",
    "    Remove punctuation, make all words lowercase, and lemmatize\n",
    "'''\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import Word\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "\n",
    "'''\n",
    "    NLTK has a model to tag words as adjectives, nouns, etc,\n",
    "    but NLTK uses wordnet for lemmatization. wordnet only uses\n",
    "    four possible tags, while NLTK returns tons of unique ones\n",
    "\n",
    "    This function transforms NLTK tags to wordnet tags for lemmatization\n",
    "'''\n",
    "def nltk_tag_to_wordnet(tag: str) -> str:\n",
    "    if tag[0] == \"J\":\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0] == \"V\":\n",
    "        return wordnet.VERB\n",
    "    elif tag[0] == \"N\":\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0] == \"R\":\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "'''\n",
    "    Remove non-alphabetical characters and punctuation\n",
    "'''\n",
    "def keep_only_alphabetic(s: str) -> str:\n",
    "    temp = \"\".join([\" \" if i in string.punctuation else i for i in s])\n",
    "    return \"\".join([i for i in temp if (ord(i) <= 90 and ord(i) >= 65) or (ord(i) <= 122 and ord(i) >= 97) or i.isspace()])\n",
    "\n",
    "'''\n",
    "    Take a string of text, tokenize it, and return a list of lemmatized tokens\n",
    "'''\n",
    "def lemmatize_words(s: str) -> list[str]:\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    words = [i.lower() for i in word_tokenize(s)]       # tokenize and lowercase\n",
    "    words = [i for i in words if i not in stopwords.words(\"english\")]   # remove stopwords\n",
    "    words = list(filter(lambda x: nltk_tag_to_wordnet(x[1]) !=\"\", pos_tag(words)))  # remove invalid lemmatization words and tags\n",
    "    words = [lemmer.lemmatize(i[0], nltk_tag_to_wordnet(i[1])) for i in words]  #  lemmatize words\n",
    "    return words\n",
    "\n",
    "'''\n",
    "    Combine all functions above to pre-process strng\n",
    "'''\n",
    "def pre_process_text(text: str) -> str:\n",
    "    s = keep_only_alphabetic(text)\n",
    "    lemmatized = lemmatize_words(s)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_process_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pre_process_text(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:9565\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9554\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9556\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9557\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9558\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9563\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9564\u001b[0m )\n\u001b[1;32m-> 9565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 873\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    891\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    892\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    893\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mpre_process_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pre_process_text(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 55\u001b[0m, in \u001b[0;36mpre_process_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpre_process_text\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     54\u001b[0m     s \u001b[38;5;241m=\u001b[39m keep_only_alphabetic(text)\n\u001b[1;32m---> 55\u001b[0m     lemmatized \u001b[38;5;241m=\u001b[39m \u001b[43mlemmatize_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatized)\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36mlemmatize_words\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     43\u001b[0m lemmer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mWordNetLemmatizer()\n\u001b[0;32m     44\u001b[0m words \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m word_tokenize(s)]       \u001b[38;5;66;03m# tokenize and lowercase\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m   \u001b[38;5;66;03m# remove stopwords\u001b[39;00m\n\u001b[0;32m     46\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: nltk_tag_to_wordnet(x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_tag(words)))  \u001b[38;5;66;03m# remove invalid lemmatization words and tags\u001b[39;00m\n\u001b[0;32m     47\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmer\u001b[38;5;241m.\u001b[39mlemmatize(i[\u001b[38;5;241m0\u001b[39m], nltk_tag_to_wordnet(i[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m words]  \u001b[38;5;66;03m#  lemmatize words\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     43\u001b[0m lemmer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mWordNetLemmatizer()\n\u001b[0;32m     44\u001b[0m words \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m word_tokenize(s)]       \u001b[38;5;66;03m# tokenize and lowercase\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m words \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]   \u001b[38;5;66;03m# remove stopwords\u001b[39;00m\n\u001b[0;32m     46\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: nltk_tag_to_wordnet(x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, pos_tag(words)))  \u001b[38;5;66;03m# remove invalid lemmatization words and tags\u001b[39;00m\n\u001b[0;32m     47\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmer\u001b[38;5;241m.\u001b[39mlemmatize(i[\u001b[38;5;241m0\u001b[39m], nltk_tag_to_wordnet(i[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m words]  \u001b[38;5;66;03m#  lemmatize words\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[0m, in \u001b[0;36mWordListCorpusReader.words\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m     20\u001b[0m         line\n\u001b[1;32m---> 21\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(ignore_lines_startswith)\n\u001b[0;32m     23\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[0m, in \u001b[0;36mCorpusReader.raw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m    216\u001b[0m contents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    219\u001b[0m         contents\u001b[38;5;241m.\u001b[39mappend(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[0m, in \u001b[0;36mCorpusReader.open\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    230\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding(file)\n\u001b[1;32m--> 231\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(encoding)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:333\u001b[0m, in \u001b[0;36mFileSystemPathPointer.join\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[0;32m    332\u001b[0m     _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path, fileid)\n\u001b[1;32m--> 333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\data.py:310\u001b[0m, in \u001b[0;36mFileSystemPathPointer.__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03mCreate a new path pointer for the given absolute path.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m:raise IOError: If the given path does not exist.\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    309\u001b[0m _path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_path)\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m _path)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m _path\n",
      "File \u001b[1;32m<frozen genericpath>:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data[\"text\"] = data.apply(lambda x: pre_process_text(x[\"text\"]), axis=1)\n",
    "data[\"title\"] = data.apply(lambda x: pre_process_text(x[\"title\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP FOR WELFAKE\n",
    "\n",
    "'''\n",
    "def to_class(s):\n",
    "    if s == \"real\":\n",
    "        return 1\n",
    "    return 0\n",
    "'''\n",
    "\n",
    "\n",
    "def liar_class_process(s: str) -> int:\n",
    "    if s.lower() == \"true\":\n",
    "        return 1\n",
    "    elif s.lower() == \"mostly-true\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"flag\"] = data.apply(lambda x: to_class(x[\"flag\"]), axis=1)\n",
    "#data\n",
    "\n",
    "if \"liar\" in read_file.lower():\n",
    "    data[\"class\"] = data.apply(lambda x: liar_class_process(x[\"class\"]), axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_tb_pol\"] = data.apply(lambda x: TextBlob(x[\"text\"]).polarity, axis=1)\n",
    "data[\"text_tb_sub\"] = data.apply(lambda x: TextBlob(x[\"text\"]).subjectivity, axis=1)\n",
    "\n",
    "data[\"title_tb_pol\"] = data.apply(lambda x: TextBlob(x[\"title\"]).polarity, axis=1)\n",
    "data[\"title_tb_sub\"] = data.apply(lambda x: TextBlob(x[\"title\"]).subjectivity, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "data[\"title_vader_scores\"] = data.apply(lambda x: analyzer.polarity_scores(x[\"title\"]), axis=1)\n",
    "data[\"title_vader_comp\"] = data.apply(lambda x: x[\"title_vader_scores\"][\"compound\"], axis=1)\n",
    "data[\"title_vader_neg\"] = data.apply(lambda x: x[\"title_vader_scores\"][\"neg\"], axis=1)\n",
    "data[\"title_vader_neu\"] = data.apply(lambda x: x[\"title_vader_scores\"][\"neu\"], axis=1)\n",
    "data[\"title_vader_pos\"] = data.apply(lambda x: x[\"title_vader_scores\"][\"pos\"], axis=1)\n",
    "data = data.drop([\"title_vader_scores\"], axis=1)\n",
    "\n",
    "data[\"text_vader_scores\"] = data.apply(lambda x: analyzer.polarity_scores(x[\"text\"]), axis=1)\n",
    "data[\"text_vader_comp\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"compound\"], axis=1)\n",
    "data[\"text_vader_neg\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"neg\"], axis=1)\n",
    "data[\"text_vader_neu\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"neu\"], axis=1)\n",
    "data[\"text_vader_pos\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"pos\"], axis=1)\n",
    "data = data.drop([\"text_vader_scores\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_vader(neg, neu, pos):\n",
    "    return np.argmax([neg, neu, pos])\n",
    "    \n",
    "def discrete_textblob(pol):\n",
    "    if pol <= -0.33:\n",
    "        return 0\n",
    "    elif pol > -0.33 and pol < 0.33:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def discrete_textblob_sub(pol):\n",
    "    if pol < 0.5:\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"title_vader_class\"] = data.apply(lambda x: discrete_vader(x[\"title_vader_neg\"], x[\"title_vader_neu\"], x[\"title_vader_pos\"]), axis=1)\n",
    "data[\"text_vader_class\"] = data.apply(lambda x: discrete_vader(x[\"text_vader_neg\"], x[\"text_vader_neu\"], x[\"text_vader_pos\"]), axis=1)\n",
    "\n",
    "data[\"text_tb_pol_class\"] = data.apply(lambda x: discrete_textblob(x[\"text_tb_pol\"]), axis=1)\n",
    "data[\"text_tb_sub_class\"] = data.apply(lambda x: discrete_textblob_sub(x[\"text_tb_sub\"]), axis=1)\n",
    "\n",
    "data[\"title_tb_pol_class\"] = data.apply(lambda x: discrete_textblob(x[\"title_tb_pol\"]), axis=1)\n",
    "data[\"title_tb_sub_class\"] = data.apply(lambda x: discrete_textblob_sub(x[\"title_tb_sub\"]), axis=1)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(r\"../data/Des_fake_news/ISOT_PROCESSED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Begin Neural Net for news text\n",
    "'''\n",
    "\n",
    "#data = pd.read_csv(r\"../data/Des_fake_news/ISOT_PROCESSED.csv\")\n",
    "'''text only'''\n",
    "features = data[[\"text_tb_pol\", \"text_vader_pos\", \"text_vader_neg\", \"text_vader_neu\"]].to_numpy()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "imdb_model = tf.keras.models.load_model(r\"SA_imdb/IMDB_NN.keras\")\n",
    "imdb_sentiment = np.argmax(imdb_model.predict(features), axis=-1)\n",
    "\n",
    "tweets_model = tf.keras.models.load_model(r\"SA_tweets/TWEETS_NN.keras\")\n",
    "tweets_sentiment = np.argmax(tweets_model.predict(features), axis=-1)\n",
    "\n",
    "data[\"text_NN_imdb\"] = imdb_sentiment\n",
    "data[\"text_NN_tweets\"] = tweets_sentiment\n",
    "\n",
    "#data.to_csv(r\"../data/Des_fake_news/ISOT_PROCESSED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Begin Neural Net for news title\n",
    "'''\n",
    "\n",
    "\n",
    "'''text only'''\n",
    "features = data[[\"title_tb_pol\", \"title_vader_pos\", \"title_vader_neg\", \"title_vader_neu\"]].to_numpy()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "imdb_model_two = tf.keras.models.load_model(r\"SA_imdb/IMDB_NN.keras\")\n",
    "imdb_sentiment_two = np.argmax(imdb_model_two.predict(features), axis=-1)\n",
    "\n",
    "tweets_model_two = tf.keras.models.load_model(r\"SA_tweets/TWEETS_NN.keras\")\n",
    "tweets_sentiment_two = np.argmax(tweets_model_two.predict(features), axis=-1)\n",
    "\n",
    "data[\"title_NN_imdb\"] = imdb_sentiment_two\n",
    "data[\"title_NN_tweets\"] = tweets_sentiment_two\n",
    "\n",
    "#data.to_csv(r\"../data/Des_fake_news/ISOT_PROCESSED.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Begin log regression for text\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "texts = data[[\"text_tb_pol\", \"text_vader_pos\", \"text_vader_neg\", \"text_vader_neu\"]].to_numpy()\n",
    "\n",
    "tweet_text_log:  LogisticRegression = None\n",
    "with open(r\"SA_tweets/TWEETS_LOG.pkl\", \"rb\") as file:\n",
    "    tweet_text_log = pickle.load(file)\n",
    "\n",
    "imdb_text_log:  LogisticRegression = None\n",
    "with open(r\"SA_imdb/IMDB_LOG.pkl\", \"rb\") as file:\n",
    "    imdb_text_log = pickle.load(file)\n",
    "\n",
    "data[\"text_log_imdb\"] = imdb_text_log.predict(texts)\n",
    "data[\"text_log_tweets\"] = imdb_text_log.predict(texts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Begin log regression for title\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "titles = data[[\"title_tb_pol\", \"title_vader_pos\", \"title_vader_neg\", \"title_vader_neu\"]].to_numpy()\n",
    "\n",
    "\n",
    "tweet_text_log:  LogisticRegression = None\n",
    "with open(r\"SA_tweets/TWEETS_LOG.pkl\", \"rb\") as file:\n",
    "    tweet_text_log = pickle.load(file)\n",
    "\n",
    "imdb_text_log:  LogisticRegression = None\n",
    "with open(r\"SA_imdb/IMDB_LOG.pkl\", \"rb\") as file:\n",
    "    imdb_text_log = pickle.load(file)\n",
    "\n",
    "data[\"title_log_imdb\"] = imdb_text_log.predict(texts)\n",
    "data[\"title_log_tweets\"] = imdb_text_log.predict(texts)\n",
    "\n",
    "\n",
    "data.to_csv(write_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(write_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
