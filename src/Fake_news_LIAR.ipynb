{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    ISOT\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/LIAR_PROCESSED_train.csv\")\n",
    "test = pd.read_csv(r\"../data/Des_fake_news/LIAR_PROCESSED_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_flag(sentiment: str):\n",
    "    if sentiment.lower() == \"true\":\n",
    "        return 4\n",
    "    elif sentiment.lower() == \"mostly-true\":\n",
    "        return 3\n",
    "    elif sentiment.lower() == \"half-true\":\n",
    "        return 2\n",
    "    elif sentiment.lower() == \"barely-true\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data[\"flag\"] = data.apply(lambda x: multi_flag(x[\"class\"]), axis=1)\n",
    "test[\"flag\"] = test.apply(lambda x: multi_flag(x[\"class\"]), axis=1)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, textblob and vader: 27.67%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([281, 157, 193, 208, 173], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Scale embeddings\n",
    "\n",
    "\n",
    "#features = training[[\"text_tb_pol\", \"text_tb_sub\", \"hash_tb_pol\", \"hash_tb_sub\", \"text_vader_neg\", \"text_vader_neu\", \"text_vader_pos\", \"hash_vader_neg\", \"hash_vader_neu\", \"hash_vader_pos\"]].to_numpy()\n",
    "\n",
    "'''text only'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"flag\"].to_numpy()\n",
    "#features = filtered[[\"title_vader_class\", \"text_vader_class\", \"text_tb_pol_class\", \"text_tb_sub_class\", \"title_tb_pol_class\", \"title_tb_sub_class\"]].dropna()\n",
    "\n",
    "'''\n",
    "  Tweet classifier\n",
    "[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", title_log_tweets\"]\n",
    "'''\n",
    "\n",
    "tweet_features = [\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\"]\n",
    "imdb_features = [\"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]\n",
    "\n",
    "tweet_and_imdb = [i for i in tweet_features]\n",
    "tweet_and_imdb.extend(imdb_features)\n",
    "\n",
    "features = filtered[[\"text_tb_pol\",\t\"text_tb_sub\",\t\n",
    "              \t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "\n",
    "#for i in range(100):\n",
    "    \n",
    "x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=42)\n",
    "#x_train\n",
    "\n",
    "ensemble = LogisticRegression()\n",
    "ensemble.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate predictions\n",
    "y_pred = ensemble.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Logistic regression, textblob and vader: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "uniques, counts = np.unique(y_test, return_counts=True)\n",
    "counts\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 - 1s - 7ms/step - accuracy: 0.2507 - loss: 1.6199\n",
      "Epoch 2/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2558 - loss: 1.6027\n",
      "Epoch 3/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2551 - loss: 1.5985\n",
      "Epoch 4/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2551 - loss: 1.5968\n",
      "Epoch 5/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2527 - loss: 1.5961\n",
      "Epoch 6/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2549 - loss: 1.5956\n",
      "Epoch 7/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2558 - loss: 1.5950\n",
      "Epoch 8/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2528 - loss: 1.5945\n",
      "Epoch 9/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2520 - loss: 1.5942\n",
      "Epoch 10/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2525 - loss: 1.5941\n",
      "Epoch 11/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2525 - loss: 1.5939\n",
      "Epoch 12/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2525 - loss: 1.5938\n",
      "Epoch 13/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2527 - loss: 1.5938\n",
      "Epoch 14/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2539 - loss: 1.5937\n",
      "Epoch 15/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2549 - loss: 1.5936\n",
      "Epoch 16/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2530 - loss: 1.5935\n",
      "Epoch 17/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2549 - loss: 1.5935\n",
      "Epoch 18/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2542 - loss: 1.5935\n",
      "Epoch 19/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2546 - loss: 1.5934\n",
      "Epoch 20/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2537 - loss: 1.5933\n",
      "Epoch 21/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2539 - loss: 1.5933\n",
      "Epoch 22/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2541 - loss: 1.5933\n",
      "Epoch 23/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2548 - loss: 1.5933\n",
      "Epoch 24/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2551 - loss: 1.5932\n",
      "Epoch 25/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2535 - loss: 1.5933\n",
      "Epoch 26/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2537 - loss: 1.5932\n",
      "Epoch 27/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2539 - loss: 1.5933\n",
      "Epoch 28/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2539 - loss: 1.5932\n",
      "Epoch 29/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2539 - loss: 1.5931\n",
      "Epoch 30/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2542 - loss: 1.5931\n",
      "Epoch 31/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2525 - loss: 1.5931\n",
      "Epoch 32/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2555 - loss: 1.5931\n",
      "Epoch 33/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2542 - loss: 1.5930\n",
      "Epoch 34/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2541 - loss: 1.5931\n",
      "Epoch 35/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2542 - loss: 1.5930\n",
      "Epoch 36/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2568 - loss: 1.5931\n",
      "Epoch 37/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2544 - loss: 1.5930\n",
      "Epoch 38/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2537 - loss: 1.5930\n",
      "Epoch 39/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2537 - loss: 1.5929\n",
      "Epoch 40/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2539 - loss: 1.5930\n",
      "Epoch 41/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2548 - loss: 1.5929\n",
      "Epoch 42/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2532 - loss: 1.5928\n",
      "Epoch 43/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2544 - loss: 1.5929\n",
      "Epoch 44/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2527 - loss: 1.5926\n",
      "Epoch 45/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2523 - loss: 1.5926\n",
      "Epoch 46/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2541 - loss: 1.5926\n",
      "Epoch 47/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2527 - loss: 1.5926\n",
      "Epoch 48/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2539 - loss: 1.5925\n",
      "Epoch 49/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2525 - loss: 1.5925\n",
      "Epoch 50/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2518 - loss: 1.5924\n",
      "Epoch 51/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2520 - loss: 1.5923\n",
      "Epoch 52/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2514 - loss: 1.5922\n",
      "Epoch 53/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2532 - loss: 1.5921\n",
      "Epoch 54/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2525 - loss: 1.5920\n",
      "Epoch 55/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2530 - loss: 1.5920\n",
      "Epoch 56/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2532 - loss: 1.5919\n",
      "Epoch 57/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2534 - loss: 1.5918\n",
      "Epoch 58/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2534 - loss: 1.5917\n",
      "Epoch 59/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2555 - loss: 1.5917\n",
      "Epoch 60/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2548 - loss: 1.5917\n",
      "Epoch 61/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2555 - loss: 1.5915\n",
      "Epoch 62/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2523 - loss: 1.5915\n",
      "Epoch 63/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2555 - loss: 1.5915\n",
      "Epoch 64/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2563 - loss: 1.5914\n",
      "Epoch 65/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2542 - loss: 1.5914\n",
      "Epoch 66/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2556 - loss: 1.5913\n",
      "Epoch 67/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2560 - loss: 1.5913\n",
      "Epoch 68/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2558 - loss: 1.5912\n",
      "Epoch 69/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2565 - loss: 1.5913\n",
      "Epoch 70/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2562 - loss: 1.5912\n",
      "Epoch 71/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2555 - loss: 1.5911\n",
      "Epoch 72/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2556 - loss: 1.5910\n",
      "Epoch 73/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2555 - loss: 1.5910\n",
      "Epoch 74/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2567 - loss: 1.5909\n",
      "Epoch 75/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2577 - loss: 1.5909\n",
      "Epoch 76/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2575 - loss: 1.5908\n",
      "Epoch 77/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2570 - loss: 1.5908\n",
      "Epoch 78/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2572 - loss: 1.5909\n",
      "Epoch 79/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2570 - loss: 1.5907\n",
      "Epoch 80/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2593 - loss: 1.5907\n",
      "Epoch 81/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2579 - loss: 1.5906\n",
      "Epoch 82/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2579 - loss: 1.5907\n",
      "Epoch 83/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2563 - loss: 1.5904\n",
      "Epoch 84/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2575 - loss: 1.5904\n",
      "Epoch 85/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2591 - loss: 1.5903\n",
      "Epoch 86/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2588 - loss: 1.5902\n",
      "Epoch 87/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2588 - loss: 1.5902\n",
      "Epoch 88/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2589 - loss: 1.5903\n",
      "Epoch 89/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2584 - loss: 1.5902\n",
      "Epoch 90/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2589 - loss: 1.5902\n",
      "Epoch 91/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2600 - loss: 1.5901\n",
      "Epoch 92/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2589 - loss: 1.5899\n",
      "Epoch 93/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2600 - loss: 1.5899\n",
      "Epoch 94/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2589 - loss: 1.5899\n",
      "Epoch 95/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2616 - loss: 1.5898\n",
      "Epoch 96/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2584 - loss: 1.5898\n",
      "Epoch 97/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2591 - loss: 1.5897\n",
      "Epoch 98/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2600 - loss: 1.5897\n",
      "Epoch 99/100\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.2586 - loss: 1.5897\n",
      "Epoch 100/100\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.2607 - loss: 1.5895\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27679194190>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(6, 1)),\n",
    "  tf.keras.layers.Dense(6, activation='relu'),\n",
    "  tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "'''text only'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"flag\"].to_numpy()\n",
    "#features = filtered[[\"title_vader_class\", \"text_vader_class\", \"text_tb_pol_class\", \"text_tb_sub_class\", \"title_tb_pol_class\", \"title_tb_sub_class\"]].to_numpy()\n",
    "features = filtered[[\"text_tb_pol\",\t\"text_tb_sub\",\t\n",
    "             \t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "'''\n",
    "  Tweet classifier\n",
    "[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", title_log_tweets\"]\n",
    "'''\n",
    "\n",
    "tweet_features = [\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\"]\n",
    "#imdb_features = [\"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]\n",
    "\n",
    "tweet_and_imdb = [i for i in tweet_features]\n",
    "tweet_and_imdb.extend(imdb_features)\n",
    "\n",
    "#features = filtered[tweet_and_imdb].to_numpy()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=42)\n",
    "model.fit(x_train, y_train, epochs=100, verbose=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log LIAR: 24.42%\n",
      "27/27 - 0s - 3ms/step - accuracy: 0.2431 - loss: 1.6023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24305555555555555"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "filtered_2 = test.dropna()\n",
    "\n",
    "\n",
    "x_true_test = filtered_2[[\"text_tb_pol\",\t\"text_tb_sub\",\t\n",
    "              \t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "y_true_vals = filtered_2[\"flag\"].to_numpy()\n",
    "y_log_pred = ensemble.predict(x_true_test)\n",
    "accuracy = accuracy_score(y_log_pred, y_true_vals)\n",
    "print(f\"Log LIAR: {accuracy * 100:.2f}%\")\n",
    "results = model.evaluate(x_true_test,  y_true_vals, verbose=2)\n",
    "\n",
    "matrix = confusion_matrix(y_true_vals, y_log_pred)\n",
    "matrix\n",
    "\n",
    "\n",
    "uniques, counts = np.unique(y_true_vals, return_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
