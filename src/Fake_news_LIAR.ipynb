{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    LIAR\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/LIAR_PROCESSED_2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>barelytrues</th>\n",
       "      <th>...</th>\n",
       "      <th>__word327</th>\n",
       "      <th>__word328</th>\n",
       "      <th>__word329</th>\n",
       "      <th>__word330</th>\n",
       "      <th>__word331</th>\n",
       "      <th>__word332</th>\n",
       "      <th>__word333</th>\n",
       "      <th>__word334</th>\n",
       "      <th>__word335</th>\n",
       "      <th>__word336</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11972.json</td>\n",
       "      <td>4</td>\n",
       "      <td>Building a wall on the U S  Mexico border will...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11685.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11096.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>military veterans voting record</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>President-Elect</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5209.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
       "      <td>medicare message machine  campaign advertising</td>\n",
       "      <td>rob-cornilles</td>\n",
       "      <td>consultant</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>republican</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7070.json</td>\n",
       "      <td>4</td>\n",
       "      <td>Says that Tennessee law requires that schools ...</td>\n",
       "      <td>county budget county government education taxes</td>\n",
       "      <td>stand-children-tennessee</td>\n",
       "      <td>Child and education advocacy organization.</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.224453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>8463</td>\n",
       "      <td>7013.json</td>\n",
       "      <td>2</td>\n",
       "      <td>Says U S  Rep  Charles Bass wants to privatize...</td>\n",
       "      <td>social security</td>\n",
       "      <td>ann-mclane-kuster</td>\n",
       "      <td>Attorney</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>8464</td>\n",
       "      <td>2661.json</td>\n",
       "      <td>1</td>\n",
       "      <td>In the past two years  Democrats have spent mo...</td>\n",
       "      <td>federal budget history</td>\n",
       "      <td>eric-cantor</td>\n",
       "      <td>House Majority Leader</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>republican</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>8465</td>\n",
       "      <td>3419.json</td>\n",
       "      <td>0</td>\n",
       "      <td>For the first time in more than a decade  impo...</td>\n",
       "      <td>energy oil spill trade</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>8466</td>\n",
       "      <td>12548.json</td>\n",
       "      <td>3</td>\n",
       "      <td>Says Donald Trump has bankrupted his companies...</td>\n",
       "      <td>candidates biography</td>\n",
       "      <td>hillary-clinton</td>\n",
       "      <td>Presidential candidate</td>\n",
       "      <td>New York</td>\n",
       "      <td>democrat</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>8467</td>\n",
       "      <td>9117.json</td>\n",
       "      <td>2</td>\n",
       "      <td>No one claims the report vindicating New Jerse...</td>\n",
       "      <td>candidates biography infrastructure</td>\n",
       "      <td>rudy-giuliani</td>\n",
       "      <td>Attorney</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8468 rows × 379 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          id  class  \\\n",
       "0              0  11972.json      4   \n",
       "1              1  11685.json      0   \n",
       "2              2  11096.json      0   \n",
       "3              3   5209.json      0   \n",
       "4              4   7070.json      4   \n",
       "...          ...         ...    ...   \n",
       "8463        8463   7013.json      2   \n",
       "8464        8464   2661.json      1   \n",
       "8465        8465   3419.json      0   \n",
       "8466        8466  12548.json      3   \n",
       "8467        8467   9117.json      2   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Building a wall on the U S  Mexico border will...   \n",
       "1     Wisconsin is on pace to double the number of l...   \n",
       "2     Says John McCain has done nothing to help the ...   \n",
       "3     Suzanne Bonamici supports a plan that will cut...   \n",
       "4     Says that Tennessee law requires that schools ...   \n",
       "...                                                 ...   \n",
       "8463  Says U S  Rep  Charles Bass wants to privatize...   \n",
       "8464  In the past two years  Democrats have spent mo...   \n",
       "8465  For the first time in more than a decade  impo...   \n",
       "8466  Says Donald Trump has bankrupted his companies...   \n",
       "8467  No one claims the report vindicating New Jerse...   \n",
       "\n",
       "                                                title  \\\n",
       "0                                         immigration   \n",
       "1                                                jobs   \n",
       "2                     military veterans voting record   \n",
       "3      medicare message machine  campaign advertising   \n",
       "4     county budget county government education taxes   \n",
       "...                                               ...   \n",
       "8463                                  social security   \n",
       "8464                           federal budget history   \n",
       "8465                           energy oil spill trade   \n",
       "8466                             candidates biography   \n",
       "8467              candidates biography infrastructure   \n",
       "\n",
       "                       speaker                                   job title  \\\n",
       "0                   rick-perry                                    Governor   \n",
       "1            katrina-shankland                        State representative   \n",
       "2                 donald-trump                             President-Elect   \n",
       "3                rob-cornilles                                  consultant   \n",
       "4     stand-children-tennessee  Child and education advocacy organization.   \n",
       "...                        ...                                         ...   \n",
       "8463         ann-mclane-kuster                                    Attorney   \n",
       "8464               eric-cantor                       House Majority Leader   \n",
       "8465              barack-obama                                   President   \n",
       "8466           hillary-clinton                      Presidential candidate   \n",
       "8467             rudy-giuliani                                    Attorney   \n",
       "\n",
       "              state       party  barelytrues  ...  __word327  __word328  \\\n",
       "0             Texas  republican           30  ...        0.0        0.0   \n",
       "1         Wisconsin    democrat            2  ...        0.0        0.0   \n",
       "2          New York  republican           63  ...        0.0        0.0   \n",
       "3            Oregon  republican            1  ...        0.0        0.0   \n",
       "4         Tennessee        none            0  ...        0.0        0.0   \n",
       "...             ...         ...          ...  ...        ...        ...   \n",
       "8463  New Hampshire    democrat            2  ...        0.0        0.0   \n",
       "8464       Virginia  republican            9  ...        0.0        0.0   \n",
       "8465       Illinois    democrat           70  ...        0.0        0.0   \n",
       "8466       New York    democrat           40  ...        0.0        0.0   \n",
       "8467       New York  republican            9  ...        0.0        0.0   \n",
       "\n",
       "      __word329  __word330 __word331  __word332  __word333  __word334  \\\n",
       "0           0.0        0.0       0.0        0.0   0.000000   0.311494   \n",
       "1           0.0        0.0       0.0        0.0   0.470109   0.000000   \n",
       "2           0.0        0.0       0.0        0.0   0.000000   0.000000   \n",
       "3           0.0        0.0       0.0        0.0   0.000000   0.000000   \n",
       "4           0.0        0.0       0.0        0.0   0.224453   0.000000   \n",
       "...         ...        ...       ...        ...        ...        ...   \n",
       "8463        0.0        0.0       0.0        0.0   0.000000   0.000000   \n",
       "8464        0.0        0.0       0.0        0.0   0.000000   0.383716   \n",
       "8465        0.0        0.0       0.0        0.0   0.000000   0.000000   \n",
       "8466        0.0        0.0       0.0        0.0   0.000000   0.000000   \n",
       "8467        0.0        0.0       0.0        0.0   0.000000   0.000000   \n",
       "\n",
       "      __word335  __word336  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           0.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "...         ...        ...  \n",
       "8463        0.0        0.0  \n",
       "8464        0.0        0.0  \n",
       "8465        0.0        0.0  \n",
       "8466        0.0        0.0  \n",
       "8467        0.0        0.0  \n",
       "\n",
       "[8468 rows x 379 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Add tf-idf vectorizer\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "corpus = data[\"text\"]\n",
    "vectorizer = TfidfVectorizer(strip_accents=\"ascii\", lowercase=True, stop_words=stop_words, max_features=1000, min_df=50, ngram_range=(1,3))\n",
    "features = vectorizer.fit_transform(corpus).toarray()\n",
    "names = vectorizer.get_feature_names_out()\n",
    "__names = vectorizer.get_feature_names_out()\n",
    "names = [f\"__word{i}\" for i in range(len(names))]\n",
    "feature_frame = pd.DataFrame(features, columns=names)\n",
    "feature_frame\n",
    "\n",
    "\n",
    "data = pd.concat([data, feature_frame], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "'''\n",
    "    Define filtered dataset, classes, features, dataframe for model accuracies, and excel file for results\n",
    "'''\n",
    "\n",
    "'''Drop NA'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"class\"].to_numpy()\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "#just tweet sentiment classifier\n",
    "tweet_features = [\"text_NN_tweets\", \"text_log_tweets\",  \"text_tb_sub_class\"] + names\n",
    "\n",
    "#just imbd sentiment classifier\n",
    "imdb_features = [\"text_NN_imdb\", \"text_log_imdb\"] + names\n",
    "\n",
    "# both sentiment classifiers\n",
    "tweet_and_imdb = [\"text_NN_tweets\", \"text_log_tweets\",  \"text_tb_sub_class\", \n",
    "                           \"text_NN_imdb\", \"text_log_imdb\"] + names\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "raw_features = [\"text_tb_pol\",\t\"text_tb_sub\",\n",
    "             \t\t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"] + names\n",
    "\n",
    "# dataframe to store accuracies for NN and log regression\n",
    "accuracy_df = pd.DataFrame(columns=[\"tweet_classifier\",\n",
    "               \"imdb_classifier\",\n",
    "               \"combined_classifier\",\n",
    "               \"raw_sentiments\"])\n",
    "\n",
    "\n",
    "EXCEL_FILE = r\"../data/Des_fake_news/Sentiment_Analysis_Results/LIAR_RESULTS.xlsx\"\n",
    "# overwrite book if exists\n",
    "book = Workbook()\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_18072\\3912764002.py:78: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Logistic Regression work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "logOutput = {}\n",
    "logMatrices = {\"tweet_classifier\" : [],\n",
    "               \"imdb_classifier\" : [],\n",
    "               \"combined_classifier\": [],\n",
    "               \"raw_sentiments\" : []}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "log_combined_pred = None\n",
    "log_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "tweet_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"tweet_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[imdb_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "imdb_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"imdb_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_and_imdb].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_combined_pred = lbgfs.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, log_combined_pred)\n",
    "logMatrices[\"combined_classifier\"].append(confusion_matrix(y_test, log_combined_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[raw_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_raw_pred = lbgfs.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, log_raw_pred)\n",
    "logMatrices[\"raw_sentiments\"].append(confusion_matrix(y_test, log_raw_pred))\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "logOutput[\"tweet_classifier\"] = tweet_scores / 1\n",
    "logOutput[\"imdb_classifier\"] = imdb_scores / 1\n",
    "logOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "logOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "logMatrices[\"tweet_classifier\"] = np.mean(np.array(logMatrices[\"tweet_classifier\"]), axis=0)\n",
    "logMatrices[\"imdb_classifier\"] = np.mean(np.array(logMatrices[\"imdb_classifier\"]), axis=0)\n",
    "logMatrices[\"combined_classifier\"] = np.mean(np.array(logMatrices[\"combined_classifier\"]), axis=0)\n",
    "logMatrices[\"raw_sentiments\"] = np.mean(np.array(logMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in logMatrices.keys():\n",
    "  pd.DataFrame(logMatrices[i]).to_excel(writer, sheet_name=f\"matrix_log_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Log Regression\"] = logOutput\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and save neural net models\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tweet_len = len(tweet_features)\n",
    "imdb_len = len(imdb_features)\n",
    "combined_len = len(tweet_and_imdb)\n",
    "raw_len = len(raw_features)\n",
    "\n",
    "\n",
    "tweet_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(tweet_len, 1)),\n",
    "  tf.keras.layers.Dense(tweet_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "tweet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "imdb_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(imdb_len, 1)),\n",
    "  tf.keras.layers.Dense(imdb_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "imdb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "combined_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(combined_len, 1)),\n",
    "  tf.keras.layers.Dense(combined_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "raw_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(raw_len, 1)),\n",
    "  tf.keras.layers.Dense(raw_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='softmax')\n",
    "])\n",
    "\n",
    "raw_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 2s - 7ms/step - accuracy: 0.0624 - loss: -1.4255e+01\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -4.6365e+01\n",
      "40/40 - 0s - 2ms/step - accuracy: 0.0732 - loss: -4.6203e+01\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 1s - 7ms/step - accuracy: 0.0624 - loss: -1.1596e+01\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -3.7342e+01\n",
      "40/40 - 0s - 2ms/step - accuracy: 0.0732 - loss: -3.6700e+01\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 2s - 7ms/step - accuracy: 0.0624 - loss: -1.8240e+01\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -5.9116e+01\n",
      "40/40 - 0s - 2ms/step - accuracy: 0.0732 - loss: -5.7972e+01\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 2s - 7ms/step - accuracy: 0.0624 - loss: -1.4378e+01\n",
      "Epoch 2/10\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.0624 - loss: -1.1391e+02\n",
      "Epoch 3/10\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -3.2665e+02\n",
      "Epoch 4/10\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -6.4450e+02\n",
      "Epoch 5/10\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -1.0620e+03\n",
      "Epoch 6/10\n",
      "225/225 - 1s - 2ms/step - accuracy: 0.0624 - loss: -1.5718e+03\n",
      "Epoch 7/10\n",
      "225/225 - 1s - 2ms/step - accuracy: 0.0624 - loss: -2.1681e+03\n",
      "Epoch 8/10\n",
      "225/225 - 1s - 2ms/step - accuracy: 0.0624 - loss: -2.8431e+03\n",
      "Epoch 9/10\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.0624 - loss: -3.5893e+03\n",
      "Epoch 10/10\n",
      "225/225 - 1s - 2ms/step - accuracy: 0.0624 - loss: -4.4062e+03\n",
      "225/225 - 0s - 2ms/step - accuracy: 0.0624 - loss: -4.8417e+03\n",
      "40/40 - 0s - 2ms/step - accuracy: 0.0732 - loss: -4.8668e+03\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (32, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n",
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_14236\\3925400525.py:53: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    NN results\n",
    "'''\n",
    "\n",
    "EPOCHS = 100\n",
    "nnOutput = {}\n",
    "nnMatrices = {\"tweet_classifier\" : None,\n",
    "               \"imdb_classifier\" : None,\n",
    "               \"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "\n",
    "# tweet NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "tweet_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = tweet_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = tweet_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"tweet_classifier\"] = model_acc2\n",
    "tweet_pred = [1 if i >= 0.5 else 0 for i in tweet_model.predict(x_test)]\n",
    "nnMatrices[\"tweet_classifier\"] = confusion_matrix(y_test, tweet_pred)\n",
    "\n",
    "# imdb NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[imdb_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "imdb_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = imdb_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = imdb_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"imdb_classifier\"] = model_acc2\n",
    "imdb_pred = [1 if i >= 0.5 else 0 for i in imdb_model.predict(x_test)]\n",
    "nnMatrices[\"imdb_classifier\"] = confusion_matrix(y_test, imdb_pred)\n",
    "\n",
    "# combined NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_and_imdb].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "combined_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = combined_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = combined_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"combined_classifier\"] = model_acc2\n",
    "combined_pred = [1 if i >= 0.5 else 0 for i in combined_model.predict(x_test)]\n",
    "nnMatrices[\"combined_classifier\"] = confusion_matrix(y_test, combined_pred)\n",
    "\n",
    "# raw NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[raw_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "raw_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"raw_sentiments\"] = model_acc2\n",
    "raw_pred = [1 if i >= 0.5 else 0 for i in raw_model.predict(x_test)]\n",
    "nnMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, raw_pred)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nnMatrices.keys():\n",
    "  pd.DataFrame(nnMatrices[i]).to_excel(writer, sheet_name=f\"matrix_NN_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "accuracy_df.loc[\"Neural Net\"] = nnOutput\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 0s - 1ms/step - accuracy: 0.0624 - loss: -4.8417e+03\n",
      "40/40 - 0s - 2ms/step - accuracy: 0.0732 - loss: -4.8668e+03\n",
      "Train / Test Accuracy: 6.2% / 7.3%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tCategorical feature sets for Naive Bayes and Random Forests\n",
    "'''\n",
    "\n",
    "\n",
    "# both sentiment classifiers\n",
    "categorical_tweet_and_imdb = filtered[[\"text_tb_sub_class\",  \"text_NN_imdb\", \"text_NN_tweets\",\t\n",
    "                                       \t \"text_log_imdb\", \"text_log_tweets\",  ]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "categorical_raw_features = filtered[[\"text_vader_class\", \"text_tb_pol_class\", \"text_tb_sub_class\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Naive Bayes work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "nbOutput = {}\n",
    "nbMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "nb = CategoricalNB()\n",
    "\n",
    "nb_combined_pred = None\n",
    "nb_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_combined_pred = nb.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, nb_combined_pred)\n",
    "nbMatrices[\"combined_classifier\"] = confusion_matrix(y_test, nb_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_raw_pred = nb.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, nb_raw_pred)\n",
    "nbMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, nb_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "nbOutput[\"tweet_classifier\"] = 0\n",
    "nbOutput[\"imdb_classifier\"] = 0\n",
    "nbOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "nbOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "nbMatrices[\"combined_classifier\"] = np.mean(np.array(nbMatrices[\"combined_classifier\"]), axis=0)\n",
    "nbMatrices[\"raw_sentiments\"] = np.mean(np.array(nbMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nbMatrices.keys():\n",
    "  pd.DataFrame(nbMatrices[i]).to_excel(writer, sheet_name=f\"matrix_nb_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Naive Bayes\"] = nbOutput\n",
    "\n",
    "accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Random forests work\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# average accuracies\n",
    "rfOutput = {}\n",
    "rfMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_combined_pred = None\n",
    "rf_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_combined_pred = rf.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, rf_combined_pred)\n",
    "rfMatrices[\"combined_classifier\"] = confusion_matrix(y_test, rf_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_raw_pred = rf.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, rf_raw_pred)\n",
    "rfMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, rf_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "rfOutput[\"tweet_classifier\"] = 0\n",
    "rfOutput[\"imdb_classifier\"] = 0\n",
    "rfOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "rfOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "rfMatrices[\"combined_classifier\"] = np.mean(np.array(rfMatrices[\"combined_classifier\"]), axis=0)\n",
    "rfMatrices[\"raw_sentiments\"] = np.mean(np.array(rfMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in rfMatrices.keys():\n",
    "  pd.DataFrame(rfMatrices[i]).to_excel(writer, sheet_name=f\"matrix_rf_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Random Forest\"] = rfOutput\n",
    "\n",
    "accuracy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Finally, save accuracy metrics to the spreadsheet\n",
    "'''\n",
    "\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "accuracy_df.to_excel(writer, sheet_name=f\"predicion_accuracies\")\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl.drawing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "y = y_test\n",
    "log_y = log_y_actual\n",
    "\n",
    "raw_scores = np.array(raw_pred)\n",
    "combined_scores = np.array(combined_pred)\n",
    "\n",
    "raw_fpr, raw_tpr, raw_thresh = metrics.roc_curve(y, raw_scores, pos_label=1)\n",
    "raw_roc_auc = metrics.auc(raw_fpr, raw_tpr)\n",
    "\n",
    "com_fpr, com_tpr, com_thresh = metrics.roc_curve(y, combined_scores, pos_label=1)\n",
    "com_roc_auc = metrics.auc(com_fpr, com_tpr)\n",
    "\n",
    "raw_fpr_log, raw_tpr_log, raw_thresh_log = metrics.roc_curve(y, log_raw_pred, pos_label=1)\n",
    "raw_roc_auc_log = metrics.auc(raw_fpr_log, raw_tpr_log)\n",
    "\n",
    "com_fpr_log, com_tpr_log, com_thresh_log = metrics.roc_curve(y, log_combined_pred, pos_label=1)\n",
    "com_roc_auc_log = metrics.auc(com_fpr_log, com_tpr_log)\n",
    "\n",
    "raw_fpr_nb, raw_tpr_nb, raw_thresh_nb = metrics.roc_curve(y, nb_raw_pred, pos_label=1)\n",
    "raw_roc_auc_nb = metrics.auc(raw_fpr_nb, raw_tpr_nb)\n",
    "\n",
    "com_fpr_nb, com_tpr_nb, com_thresh_nb = metrics.roc_curve(y, nb_combined_pred, pos_label=1)\n",
    "com_roc_auc_nb = metrics.auc(com_fpr_nb, com_tpr_nb)\n",
    "\n",
    "raw_fpr_rf, raw_tpr_rf, raw_thresh_rf = metrics.roc_curve(y, rf_raw_pred, pos_label=1)\n",
    "raw_roc_auc_rf = metrics.auc(raw_fpr_rf, raw_tpr_rf)\n",
    "\n",
    "com_fpr_rf, com_tpr_rf, com_thresh_rf = metrics.roc_curve(y, rf_combined_pred, pos_label=1)\n",
    "com_roc_auc_rf = metrics.auc(com_fpr_rf, com_tpr_rf)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(raw_fpr, raw_tpr,\n",
    " lw=lw, label='Raw NN (%0.2f)' % raw_roc_auc)\n",
    "plt.plot(com_fpr, com_tpr,\n",
    " lw=lw, label='Sentiment Classifier NN  (%0.2f)' % com_roc_auc)\n",
    "plt.plot(raw_fpr_log, raw_tpr_log,\n",
    " lw=lw, label='Raw Log (%0.2f)' % raw_roc_auc_log)\n",
    "plt.plot(com_fpr_log, com_tpr_log,\n",
    " lw=lw, label='Sentiment Classifier Log  (%0.2f)' % com_roc_auc_log)\n",
    "\n",
    "plt.plot(raw_fpr_nb, raw_tpr_nb,\n",
    " lw=lw, label='Raw Naive Bayes (%0.2f)' % raw_roc_auc_nb)\n",
    "plt.plot(com_fpr_nb, com_tpr_nb,\n",
    " lw=lw, label='Sentiment Classifier Naive Bayes  (%0.2f)' % com_roc_auc_nb)\n",
    "\n",
    "plt.plot(raw_fpr_rf, raw_tpr_rf,\n",
    " lw=lw, label='Raw Naive Random Forest (%0.2f)' % raw_roc_auc_rf)\n",
    "plt.plot(com_fpr_rf, com_tpr_rf,\n",
    " lw=lw, label='Sentiment Classifier Random Forest  (%0.2f)' % com_roc_auc_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# save figure as PNG\n",
    "png = io.BytesIO()\n",
    "plt.savefig(png, format=\"png\")\n",
    "\n",
    "\n",
    "# write PNG to excel file\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "ws = book.active\n",
    "\n",
    "img = openpyxl.drawing.image.Image(png)\n",
    "img.anchor = \"A1\"\n",
    "ws.add_image(img)\n",
    "book.save(filename=EXCEL_FILE)\n",
    "plt.close()\n",
    "book.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
