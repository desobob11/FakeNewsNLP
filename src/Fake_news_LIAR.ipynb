{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    LIAR\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/LIAR_PROCESSED_train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "'''\n",
    "    Define filtered dataset, classes, features, dataframe for model accuracies, and excel file for results\n",
    "'''\n",
    "\n",
    "'''Drop NA'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"flag\"].to_numpy()\n",
    "\n",
    "#just tweet sentiment classifier\n",
    "tweet_features = filtered[[\"text_NN_tweets\", \"text_log_tweets\",  \"text_tb_sub_class\"]].to_numpy()\n",
    "\n",
    "#just imbd sentiment classifier\n",
    "imdb_features = filtered[[\"text_NN_imdb\", \"text_log_imdb\"]].to_numpy()\n",
    "\n",
    "# both sentiment classifiers\n",
    "tweet_and_imdb = filtered[[\"text_NN_tweets\", \"text_log_tweets\",  \"text_tb_sub_class\", \n",
    "                           \"text_NN_imdb\", \"text_log_imdb\"]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "raw_features = filtered[[\"text_tb_pol\",\t\"text_tb_sub\",\n",
    "             \t\t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "# dataframe to store accuracies for NN and log regression\n",
    "accuracy_df = pd.DataFrame(columns=[\"tweet_classifier\",\n",
    "               \"imdb_classifier\",\n",
    "               \"combined_classifier\",\n",
    "               \"raw_sentiments\"])\n",
    "\n",
    "\n",
    "EXCEL_FILE = r\"../data/Des_fake_news/Sentiment_Analysis_Results/LIAR_RESULTS.xlsx\"\n",
    "# overwrite book if exists\n",
    "book = Workbook()\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\2003894392.py:78: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Logistic Regression work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "logOutput = {}\n",
    "logMatrices = {\"tweet_classifier\" : [],\n",
    "               \"imdb_classifier\" : [],\n",
    "               \"combined_classifier\": [],\n",
    "               \"raw_sentiments\" : []}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "log_combined_pred = None\n",
    "log_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "tweet_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"tweet_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "imdb_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"imdb_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_combined_pred = lbgfs.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, log_combined_pred)\n",
    "logMatrices[\"combined_classifier\"].append(confusion_matrix(y_test, log_combined_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_raw_pred = lbgfs.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, log_raw_pred)\n",
    "logMatrices[\"raw_sentiments\"].append(confusion_matrix(y_test, log_raw_pred))\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "logOutput[\"tweet_classifier\"] = tweet_scores / 1\n",
    "logOutput[\"imdb_classifier\"] = imdb_scores / 1\n",
    "logOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "logOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "logMatrices[\"tweet_classifier\"] = np.mean(np.array(logMatrices[\"tweet_classifier\"]), axis=0)\n",
    "logMatrices[\"imdb_classifier\"] = np.mean(np.array(logMatrices[\"imdb_classifier\"]), axis=0)\n",
    "logMatrices[\"combined_classifier\"] = np.mean(np.array(logMatrices[\"combined_classifier\"]), axis=0)\n",
    "logMatrices[\"raw_sentiments\"] = np.mean(np.array(logMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in logMatrices.keys():\n",
    "  pd.DataFrame(logMatrices[i]).to_excel(writer, sheet_name=f\"matrix_log_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Log Regression\"] = logOutput\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and save neural net models\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tweet_len = tweet_features.shape[1]\n",
    "imdb_len = imdb_features.shape[1]\n",
    "combined_len = tweet_and_imdb.shape[1]\n",
    "raw_len = raw_features.shape[1]\n",
    "\n",
    "\n",
    "tweet_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(tweet_len, 1)),\n",
    "  tf.keras.layers.Dense(tweet_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "tweet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "imdb_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(imdb_len, 1)),\n",
    "  tf.keras.layers.Dense(imdb_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "imdb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "combined_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(combined_len, 1)),\n",
    "  tf.keras.layers.Dense(combined_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "raw_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(raw_len, 1)),\n",
    "  tf.keras.layers.Dense(raw_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "raw_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 - 1s - 7ms/step - accuracy: 0.3736 - loss: 0.9503\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.3748 - loss: 0.7625\n",
      "32/32 - 0s - 2ms/step - accuracy: 0.3755 - loss: 0.7615\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "180/180 - 1s - 7ms/step - accuracy: 0.6261 - loss: 0.6765\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6671\n",
      "32/32 - 0s - 2ms/step - accuracy: 0.6235 - loss: 0.6679\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/40\n",
      "180/180 - 1s - 7ms/step - accuracy: 0.4547 - loss: 0.7806\n",
      "Epoch 2/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6109 - loss: 0.6685\n",
      "Epoch 3/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6114 - loss: 0.6651\n",
      "Epoch 4/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6114 - loss: 0.6645\n",
      "Epoch 5/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6114 - loss: 0.6640\n",
      "Epoch 6/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6114 - loss: 0.6639\n",
      "Epoch 7/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6175 - loss: 0.6634\n",
      "Epoch 8/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6173 - loss: 0.6632\n",
      "Epoch 9/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6205 - loss: 0.6629\n",
      "Epoch 10/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6264 - loss: 0.6627\n",
      "Epoch 11/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6240 - loss: 0.6624\n",
      "Epoch 12/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6624\n",
      "Epoch 13/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6621\n",
      "Epoch 14/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6621\n",
      "Epoch 15/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6619\n",
      "Epoch 16/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6618\n",
      "Epoch 17/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6618\n",
      "Epoch 18/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6617\n",
      "Epoch 19/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6615\n",
      "Epoch 20/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6613\n",
      "Epoch 21/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6615\n",
      "Epoch 22/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6614\n",
      "Epoch 23/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6613\n",
      "Epoch 24/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6613\n",
      "Epoch 25/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6612\n",
      "Epoch 26/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6612\n",
      "Epoch 27/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6611\n",
      "Epoch 28/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6613\n",
      "Epoch 29/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6611\n",
      "Epoch 30/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6612\n",
      "Epoch 31/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6610\n",
      "Epoch 32/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6611\n",
      "Epoch 33/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6610\n",
      "Epoch 34/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6612\n",
      "Epoch 35/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6612\n",
      "Epoch 36/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6609\n",
      "Epoch 37/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6608\n",
      "Epoch 38/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6608\n",
      "Epoch 39/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6611\n",
      "Epoch 40/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6609\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6607\n",
      "32/32 - 0s - 2ms/step - accuracy: 0.6235 - loss: 0.6619\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Epoch 1/40\n",
      "180/180 - 1s - 7ms/step - accuracy: 0.5018 - loss: 0.7013\n",
      "Epoch 2/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6217 - loss: 0.6648\n",
      "Epoch 3/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6247 - loss: 0.6621\n",
      "Epoch 4/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6238 - loss: 0.6610\n",
      "Epoch 5/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6243 - loss: 0.6603\n",
      "Epoch 6/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6245 - loss: 0.6600\n",
      "Epoch 7/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6252 - loss: 0.6596\n",
      "Epoch 8/40\n",
      "180/180 - 0s - 3ms/step - accuracy: 0.6262 - loss: 0.6593\n",
      "Epoch 9/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6264 - loss: 0.6593\n",
      "Epoch 10/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6264 - loss: 0.6591\n",
      "Epoch 11/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6264 - loss: 0.6590\n",
      "Epoch 12/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6259 - loss: 0.6588\n",
      "Epoch 13/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6259 - loss: 0.6587\n",
      "Epoch 14/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6585\n",
      "Epoch 15/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6586\n",
      "Epoch 16/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6586\n",
      "Epoch 17/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6585\n",
      "Epoch 18/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6262 - loss: 0.6585\n",
      "Epoch 19/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6262 - loss: 0.6584\n",
      "Epoch 20/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6584\n",
      "Epoch 21/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6262 - loss: 0.6583\n",
      "Epoch 22/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6584\n",
      "Epoch 23/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6583\n",
      "Epoch 24/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6262 - loss: 0.6583\n",
      "Epoch 25/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6584\n",
      "Epoch 26/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6262 - loss: 0.6582\n",
      "Epoch 27/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6262 - loss: 0.6582\n",
      "Epoch 28/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6262 - loss: 0.6581\n",
      "Epoch 29/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6582\n",
      "Epoch 30/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6261 - loss: 0.6581\n",
      "Epoch 31/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6581\n",
      "Epoch 32/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6582\n",
      "Epoch 33/40\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6262 - loss: 0.6581\n",
      "Epoch 34/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6257 - loss: 0.6581\n",
      "Epoch 35/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6581\n",
      "Epoch 36/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6579\n",
      "Epoch 37/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6582\n",
      "Epoch 38/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6581\n",
      "Epoch 39/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6259 - loss: 0.6580\n",
      "Epoch 40/40\n",
      "180/180 - 0s - 1ms/step - accuracy: 0.6261 - loss: 0.6581\n",
      "180/180 - 0s - 2ms/step - accuracy: 0.6259 - loss: 0.6576\n",
      "32/32 - 0s - 2ms/step - accuracy: 0.6235 - loss: 0.6608\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\1142495627.py:53: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    NN results\n",
    "'''\n",
    "\n",
    "EPOCHS = 40\n",
    "nnOutput = {}\n",
    "nnMatrices = {\"tweet_classifier\" : None,\n",
    "               \"imdb_classifier\" : None,\n",
    "               \"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "\n",
    "# tweet NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "tweet_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = tweet_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = tweet_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"tweet_classifier\"] = model_acc2\n",
    "tweet_pred = [1 if i >= 0.5 else 0 for i in tweet_model.predict(x_test)]\n",
    "nnMatrices[\"tweet_classifier\"] = confusion_matrix(y_test, tweet_pred)\n",
    "\n",
    "# imdb NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=42)\n",
    "imdb_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = imdb_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = imdb_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"imdb_classifier\"] = model_acc2\n",
    "imdb_pred = [1 if i >= 0.5 else 0 for i in imdb_model.predict(x_test)]\n",
    "nnMatrices[\"imdb_classifier\"] = confusion_matrix(y_test, imdb_pred)\n",
    "\n",
    "# combined NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "combined_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = combined_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = combined_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"combined_classifier\"] = model_acc2\n",
    "combined_pred = [1 if i >= 0.5 else 0 for i in combined_model.predict(x_test)]\n",
    "nnMatrices[\"combined_classifier\"] = confusion_matrix(y_test, combined_pred)\n",
    "\n",
    "# raw NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "raw_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"raw_sentiments\"] = model_acc2\n",
    "raw_pred = [1 if i >= 0.5 else 0 for i in raw_model.predict(x_test)]\n",
    "nnMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, raw_pred)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nnMatrices.keys():\n",
    "  pd.DataFrame(nnMatrices[i]).to_excel(writer, sheet_name=f\"matrix_NN_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "accuracy_df.loc[\"Neural Net\"] = nnOutput\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tCategorical feature sets for Naive Bayes and Random Forests\n",
    "'''\n",
    "\n",
    "\n",
    "# both sentiment classifiers\n",
    "categorical_tweet_and_imdb = filtered[[\"text_tb_sub_class\",  \"text_NN_imdb\", \"text_NN_tweets\",\t\n",
    "                                       \t \"text_log_imdb\", \"text_log_tweets\",  ]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "categorical_raw_features = filtered[[\"text_vader_class\", \"text_tb_pol_class\", \"text_tb_sub_class\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\801353151.py:65: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.623518         0.623518             0.623518   \n",
       "Neural Net              0.375494         0.623518             0.623518   \n",
       "Naive Bayes             0.000000         0.000000             0.623518   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.623518  \n",
       "Neural Net            0.623518  \n",
       "Naive Bayes           0.623518  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  Naive Bayes work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "nbOutput = {}\n",
    "nbMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "nb = CategoricalNB()\n",
    "\n",
    "nb_combined_pred = None\n",
    "nb_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_combined_pred = nb.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, nb_combined_pred)\n",
    "nbMatrices[\"combined_classifier\"] = confusion_matrix(y_test, nb_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_raw_pred = nb.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, nb_raw_pred)\n",
    "nbMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, nb_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "nbOutput[\"tweet_classifier\"] = 0\n",
    "nbOutput[\"imdb_classifier\"] = 0\n",
    "nbOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "nbOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "nbMatrices[\"combined_classifier\"] = np.mean(np.array(nbMatrices[\"combined_classifier\"]), axis=0)\n",
    "nbMatrices[\"raw_sentiments\"] = np.mean(np.array(nbMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nbMatrices.keys():\n",
    "  pd.DataFrame(nbMatrices[i]).to_excel(writer, sheet_name=f\"matrix_nb_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Naive Bayes\"] = nbOutput\n",
    "\n",
    "accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\3660210348.py:57: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.622530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.623518         0.623518             0.623518   \n",
       "Neural Net              0.375494         0.623518             0.623518   \n",
       "Naive Bayes             0.000000         0.000000             0.623518   \n",
       "Random Forest           0.000000         0.000000             0.623518   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.623518  \n",
       "Neural Net            0.623518  \n",
       "Naive Bayes           0.623518  \n",
       "Random Forest         0.622530  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Random forests work\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# average accuracies\n",
    "rfOutput = {}\n",
    "rfMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_combined_pred = None\n",
    "rf_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_combined_pred = rf.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, rf_combined_pred)\n",
    "rfMatrices[\"combined_classifier\"] = confusion_matrix(y_test, rf_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_raw_pred = rf.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, rf_raw_pred)\n",
    "rfMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, rf_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "rfOutput[\"tweet_classifier\"] = 0\n",
    "rfOutput[\"imdb_classifier\"] = 0\n",
    "rfOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "rfOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "rfMatrices[\"combined_classifier\"] = np.mean(np.array(rfMatrices[\"combined_classifier\"]), axis=0)\n",
    "rfMatrices[\"raw_sentiments\"] = np.mean(np.array(rfMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in rfMatrices.keys():\n",
    "  pd.DataFrame(rfMatrices[i]).to_excel(writer, sheet_name=f\"matrix_rf_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Random Forest\"] = rfOutput\n",
    "\n",
    "accuracy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\3651036480.py:7: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Finally, save accuracy metrics to the spreadsheet\n",
    "'''\n",
    "\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "accuracy_df.to_excel(writer, sheet_name=f\"predicion_accuracies\")\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl.drawing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "y = y_test\n",
    "log_y = log_y_actual\n",
    "\n",
    "raw_scores = np.array(raw_pred)\n",
    "combined_scores = np.array(combined_pred)\n",
    "\n",
    "raw_fpr, raw_tpr, raw_thresh = metrics.roc_curve(y, raw_scores, pos_label=1)\n",
    "raw_roc_auc = metrics.auc(raw_fpr, raw_tpr)\n",
    "\n",
    "com_fpr, com_tpr, com_thresh = metrics.roc_curve(y, combined_scores, pos_label=1)\n",
    "com_roc_auc = metrics.auc(com_fpr, com_tpr)\n",
    "\n",
    "raw_fpr_log, raw_tpr_log, raw_thresh_log = metrics.roc_curve(y, log_raw_pred, pos_label=1)\n",
    "raw_roc_auc_log = metrics.auc(raw_fpr_log, raw_tpr_log)\n",
    "\n",
    "com_fpr_log, com_tpr_log, com_thresh_log = metrics.roc_curve(y, log_combined_pred, pos_label=1)\n",
    "com_roc_auc_log = metrics.auc(com_fpr_log, com_tpr_log)\n",
    "\n",
    "raw_fpr_nb, raw_tpr_nb, raw_thresh_nb = metrics.roc_curve(y, nb_raw_pred, pos_label=1)\n",
    "raw_roc_auc_nb = metrics.auc(raw_fpr_nb, raw_tpr_nb)\n",
    "\n",
    "com_fpr_nb, com_tpr_nb, com_thresh_nb = metrics.roc_curve(y, nb_combined_pred, pos_label=1)\n",
    "com_roc_auc_nb = metrics.auc(com_fpr_nb, com_tpr_nb)\n",
    "\n",
    "raw_fpr_rf, raw_tpr_rf, raw_thresh_rf = metrics.roc_curve(y, rf_raw_pred, pos_label=1)\n",
    "raw_roc_auc_rf = metrics.auc(raw_fpr_rf, raw_tpr_rf)\n",
    "\n",
    "com_fpr_rf, com_tpr_rf, com_thresh_rf = metrics.roc_curve(y, rf_combined_pred, pos_label=1)\n",
    "com_roc_auc_rf = metrics.auc(com_fpr_rf, com_tpr_rf)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(raw_fpr, raw_tpr,\n",
    " lw=lw, label='Raw NN (%0.2f)' % raw_roc_auc)\n",
    "plt.plot(com_fpr, com_tpr,\n",
    " lw=lw, label='Sentiment Classifier NN  (%0.2f)' % com_roc_auc)\n",
    "plt.plot(raw_fpr_log, raw_tpr_log,\n",
    " lw=lw, label='Raw Log (%0.2f)' % raw_roc_auc_log)\n",
    "plt.plot(com_fpr_log, com_tpr_log,\n",
    " lw=lw, label='Sentiment Classifier Log  (%0.2f)' % com_roc_auc_log)\n",
    "\n",
    "plt.plot(raw_fpr_nb, raw_tpr_nb,\n",
    " lw=lw, label='Raw Naive Bayes (%0.2f)' % raw_roc_auc_nb)\n",
    "plt.plot(com_fpr_nb, com_tpr_nb,\n",
    " lw=lw, label='Sentiment Classifier Naive Bayes  (%0.2f)' % com_roc_auc_nb)\n",
    "\n",
    "plt.plot(raw_fpr_rf, raw_tpr_rf,\n",
    " lw=lw, label='Raw Naive Random Forest (%0.2f)' % raw_roc_auc_rf)\n",
    "plt.plot(com_fpr_rf, com_tpr_rf,\n",
    " lw=lw, label='Sentiment Classifier Random Forest  (%0.2f)' % com_roc_auc_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# save figure as PNG\n",
    "png = io.BytesIO()\n",
    "plt.savefig(png, format=\"png\")\n",
    "\n",
    "\n",
    "# write PNG to excel file\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "ws = book.active\n",
    "\n",
    "img = openpyxl.drawing.image.Image(png)\n",
    "img.anchor = \"A1\"\n",
    "ws.add_image(img)\n",
    "book.save(filename=EXCEL_FILE)\n",
    "plt.close()\n",
    "book.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
