{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    LIAR\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/LIAR_PROCESSED_2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>speaker</th>\n",
       "      <th>job title</th>\n",
       "      <th>state</th>\n",
       "      <th>party</th>\n",
       "      <th>barelytrues</th>\n",
       "      <th>...</th>\n",
       "      <th>__word990</th>\n",
       "      <th>__word991</th>\n",
       "      <th>__word992</th>\n",
       "      <th>__word993</th>\n",
       "      <th>__word994</th>\n",
       "      <th>__word995</th>\n",
       "      <th>__word996</th>\n",
       "      <th>__word997</th>\n",
       "      <th>__word998</th>\n",
       "      <th>__word999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11972.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Building a wall on the U S  Mexico border will...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11685.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>11096.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>military veterans voting record</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>President-Elect</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5209.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Suzanne Bonamici supports a plan that will cut...</td>\n",
       "      <td>medicare message machine  campaign advertising</td>\n",
       "      <td>rob-cornilles</td>\n",
       "      <td>consultant</td>\n",
       "      <td>Oregon</td>\n",
       "      <td>republican</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7070.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Says that Tennessee law requires that schools ...</td>\n",
       "      <td>county budget county government education taxes</td>\n",
       "      <td>stand-children-tennessee</td>\n",
       "      <td>Child and education advocacy organization.</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>8463</td>\n",
       "      <td>7013.json</td>\n",
       "      <td>0</td>\n",
       "      <td>Says U S  Rep  Charles Bass wants to privatize...</td>\n",
       "      <td>social security</td>\n",
       "      <td>ann-mclane-kuster</td>\n",
       "      <td>Attorney</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8464</th>\n",
       "      <td>8464</td>\n",
       "      <td>2661.json</td>\n",
       "      <td>0</td>\n",
       "      <td>In the past two years  Democrats have spent mo...</td>\n",
       "      <td>federal budget history</td>\n",
       "      <td>eric-cantor</td>\n",
       "      <td>House Majority Leader</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>republican</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.401585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>8465</td>\n",
       "      <td>3419.json</td>\n",
       "      <td>0</td>\n",
       "      <td>For the first time in more than a decade  impo...</td>\n",
       "      <td>energy oil spill trade</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>8466</td>\n",
       "      <td>12548.json</td>\n",
       "      <td>1</td>\n",
       "      <td>Says Donald Trump has bankrupted his companies...</td>\n",
       "      <td>candidates biography</td>\n",
       "      <td>hillary-clinton</td>\n",
       "      <td>Presidential candidate</td>\n",
       "      <td>New York</td>\n",
       "      <td>democrat</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8467</th>\n",
       "      <td>8467</td>\n",
       "      <td>9117.json</td>\n",
       "      <td>0</td>\n",
       "      <td>No one claims the report vindicating New Jerse...</td>\n",
       "      <td>candidates biography infrastructure</td>\n",
       "      <td>rudy-giuliani</td>\n",
       "      <td>Attorney</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8468 rows × 1042 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0          id  class  \\\n",
       "0              0  11972.json      1   \n",
       "1              1  11685.json      0   \n",
       "2              2  11096.json      0   \n",
       "3              3   5209.json      0   \n",
       "4              4   7070.json      1   \n",
       "...          ...         ...    ...   \n",
       "8463        8463   7013.json      0   \n",
       "8464        8464   2661.json      0   \n",
       "8465        8465   3419.json      0   \n",
       "8466        8466  12548.json      1   \n",
       "8467        8467   9117.json      0   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Building a wall on the U S  Mexico border will...   \n",
       "1     Wisconsin is on pace to double the number of l...   \n",
       "2     Says John McCain has done nothing to help the ...   \n",
       "3     Suzanne Bonamici supports a plan that will cut...   \n",
       "4     Says that Tennessee law requires that schools ...   \n",
       "...                                                 ...   \n",
       "8463  Says U S  Rep  Charles Bass wants to privatize...   \n",
       "8464  In the past two years  Democrats have spent mo...   \n",
       "8465  For the first time in more than a decade  impo...   \n",
       "8466  Says Donald Trump has bankrupted his companies...   \n",
       "8467  No one claims the report vindicating New Jerse...   \n",
       "\n",
       "                                                title  \\\n",
       "0                                         immigration   \n",
       "1                                                jobs   \n",
       "2                     military veterans voting record   \n",
       "3      medicare message machine  campaign advertising   \n",
       "4     county budget county government education taxes   \n",
       "...                                               ...   \n",
       "8463                                  social security   \n",
       "8464                           federal budget history   \n",
       "8465                           energy oil spill trade   \n",
       "8466                             candidates biography   \n",
       "8467              candidates biography infrastructure   \n",
       "\n",
       "                       speaker                                   job title  \\\n",
       "0                   rick-perry                                    Governor   \n",
       "1            katrina-shankland                        State representative   \n",
       "2                 donald-trump                             President-Elect   \n",
       "3                rob-cornilles                                  consultant   \n",
       "4     stand-children-tennessee  Child and education advocacy organization.   \n",
       "...                        ...                                         ...   \n",
       "8463         ann-mclane-kuster                                    Attorney   \n",
       "8464               eric-cantor                       House Majority Leader   \n",
       "8465              barack-obama                                   President   \n",
       "8466           hillary-clinton                      Presidential candidate   \n",
       "8467             rudy-giuliani                                    Attorney   \n",
       "\n",
       "              state       party  barelytrues  ...  __word990  __word991  \\\n",
       "0             Texas  republican           30  ...        0.0        0.0   \n",
       "1         Wisconsin    democrat            2  ...        0.0        0.0   \n",
       "2          New York  republican           63  ...        0.0        0.0   \n",
       "3            Oregon  republican            1  ...        0.0        0.0   \n",
       "4         Tennessee        none            0  ...        0.0        0.0   \n",
       "...             ...         ...          ...  ...        ...        ...   \n",
       "8463  New Hampshire    democrat            2  ...        0.0        0.0   \n",
       "8464       Virginia  republican            9  ...        0.0        0.0   \n",
       "8465       Illinois    democrat           70  ...        0.0        0.0   \n",
       "8466       New York    democrat           40  ...        0.0        0.0   \n",
       "8467       New York  republican            9  ...        0.0        0.0   \n",
       "\n",
       "      __word992  __word993 __word994  __word995  __word996  __word997  \\\n",
       "0           0.0   0.000000  0.242457        0.0        0.0        0.0   \n",
       "1           0.0   0.362282  0.000000        0.0        0.0        0.0   \n",
       "2           0.0   0.000000  0.000000        0.0        0.0        0.0   \n",
       "3           0.0   0.000000  0.000000        0.0        0.0        0.0   \n",
       "4           0.0   0.196240  0.000000        0.0        0.0        0.0   \n",
       "...         ...        ...       ...        ...        ...        ...   \n",
       "8463        0.0   0.000000  0.000000        0.0        0.0        0.0   \n",
       "8464        0.0   0.000000  0.401585        0.0        0.0        0.0   \n",
       "8465        0.0   0.000000  0.000000        0.0        0.0        0.0   \n",
       "8466        0.0   0.000000  0.000000        0.0        0.0        0.0   \n",
       "8467        0.0   0.000000  0.000000        0.0        0.0        0.0   \n",
       "\n",
       "      __word998  __word999  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           0.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "...         ...        ...  \n",
       "8463        0.0        0.0  \n",
       "8464        0.0        0.0  \n",
       "8465        0.0        0.0  \n",
       "8466        0.0        0.0  \n",
       "8467        0.0        0.0  \n",
       "\n",
       "[8468 rows x 1042 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Add tf-idf vectorizer\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "corpus = data[\"text\"]\n",
    "vectorizer = TfidfVectorizer(strip_accents=\"ascii\", lowercase=True, stop_words=stop_words, max_features=1000)\n",
    "features = vectorizer.fit_transform(corpus).toarray()\n",
    "names = vectorizer.get_feature_names_out()\n",
    "names = [f\"__word{i}\" for i in range(len(names))]\n",
    "feature_frame = pd.DataFrame(features, columns=names)\n",
    "feature_frame\n",
    "\n",
    "\n",
    "data = pd.concat([data, feature_frame], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "'''\n",
    "    Define filtered dataset, classes, features, dataframe for model accuracies, and excel file for results\n",
    "'''\n",
    "\n",
    "'''Drop NA'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"class\"].to_numpy()\n",
    "\n",
    "names = list(names)\n",
    "\n",
    "#just tweet sentiment classifier\n",
    "tweet_features = [\"text_NN_tweets\", \"text_log_tweets\",  \"text_tb_sub_class\"] + names\n",
    "\n",
    "#just imbd sentiment classifier\n",
    "imdb_features = [\"text_NN_imdb\", \"text_log_imdb\"] + names\n",
    "\n",
    "# both sentiment classifiers\n",
    "tweet_and_imdb = [\"text_NN_tweets\", \"text_log_tweets\",  \"text_tb_sub_class\", \n",
    "                           \"text_NN_imdb\", \"text_log_imdb\"] + names\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "raw_features = [\"text_tb_pol\",\t\"text_tb_sub\",\n",
    "             \t\t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"] + names\n",
    "\n",
    "# dataframe to store accuracies for NN and log regression\n",
    "accuracy_df = pd.DataFrame(columns=[\"tweet_classifier\",\n",
    "               \"imdb_classifier\",\n",
    "               \"combined_classifier\",\n",
    "               \"raw_sentiments\"])\n",
    "\n",
    "\n",
    "EXCEL_FILE = r\"../data/Des_fake_news/Sentiment_Analysis_Results/LIAR_RESULTS.xlsx\"\n",
    "# overwrite book if exists\n",
    "book = Workbook()\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_5880\\3912764002.py:78: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Logistic Regression work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "logOutput = {}\n",
    "logMatrices = {\"tweet_classifier\" : [],\n",
    "               \"imdb_classifier\" : [],\n",
    "               \"combined_classifier\": [],\n",
    "               \"raw_sentiments\" : []}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "log_combined_pred = None\n",
    "log_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "tweet_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"tweet_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[imdb_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "imdb_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"imdb_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_and_imdb].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_combined_pred = lbgfs.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, log_combined_pred)\n",
    "logMatrices[\"combined_classifier\"].append(confusion_matrix(y_test, log_combined_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[raw_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_raw_pred = lbgfs.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, log_raw_pred)\n",
    "logMatrices[\"raw_sentiments\"].append(confusion_matrix(y_test, log_raw_pred))\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "logOutput[\"tweet_classifier\"] = tweet_scores / 1\n",
    "logOutput[\"imdb_classifier\"] = imdb_scores / 1\n",
    "logOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "logOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "logMatrices[\"tweet_classifier\"] = np.mean(np.array(logMatrices[\"tweet_classifier\"]), axis=0)\n",
    "logMatrices[\"imdb_classifier\"] = np.mean(np.array(logMatrices[\"imdb_classifier\"]), axis=0)\n",
    "logMatrices[\"combined_classifier\"] = np.mean(np.array(logMatrices[\"combined_classifier\"]), axis=0)\n",
    "logMatrices[\"raw_sentiments\"] = np.mean(np.array(logMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in logMatrices.keys():\n",
    "  pd.DataFrame(logMatrices[i]).to_excel(writer, sheet_name=f\"matrix_log_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Log Regression\"] = logOutput\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and save neural net models\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tweet_len = len(tweet_features)\n",
    "imdb_len = len(imdb_features)\n",
    "combined_len = len(tweet_and_imdb)\n",
    "raw_len = len(raw_features)\n",
    "\n",
    "\n",
    "tweet_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(tweet_len, 1)),\n",
    "  tf.keras.layers.Dense(tweet_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "tweet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "imdb_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(imdb_len, 1)),\n",
    "  tf.keras.layers.Dense(imdb_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "imdb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "combined_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(combined_len, 1)),\n",
    "  tf.keras.layers.Dense(combined_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "raw_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(raw_len, 1)),\n",
    "  tf.keras.layers.Dense(raw_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "raw_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 4s - 18ms/step - accuracy: 0.6335 - loss: 0.6459\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.7113 - loss: 0.5779\n",
      "40/40 - 0s - 3ms/step - accuracy: 0.6145 - loss: 0.6479\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "225/225 - 4s - 19ms/step - accuracy: 0.6354 - loss: 0.6450\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.7000 - loss: 0.5712\n",
      "40/40 - 0s - 3ms/step - accuracy: 0.6223 - loss: 0.6489\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Epoch 1/40\n",
      "225/225 - 4s - 18ms/step - accuracy: 0.6317 - loss: 0.6457\n",
      "Epoch 2/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.6913 - loss: 0.5802\n",
      "Epoch 3/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.7473 - loss: 0.5149\n",
      "Epoch 4/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.8020 - loss: 0.4348\n",
      "Epoch 5/40\n",
      "225/225 - 5s - 23ms/step - accuracy: 0.8609 - loss: 0.3463\n",
      "Epoch 6/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9125 - loss: 0.2558\n",
      "Epoch 7/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9544 - loss: 0.1784\n",
      "Epoch 8/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9811 - loss: 0.1176\n",
      "Epoch 9/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9919 - loss: 0.0781\n",
      "Epoch 10/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9953 - loss: 0.0532\n",
      "Epoch 11/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9969 - loss: 0.0379\n",
      "Epoch 12/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9967 - loss: 0.0290\n",
      "Epoch 13/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0227\n",
      "Epoch 14/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9969 - loss: 0.0195\n",
      "Epoch 15/40\n",
      "225/225 - 5s - 23ms/step - accuracy: 0.9968 - loss: 0.0174\n",
      "Epoch 16/40\n",
      "225/225 - 5s - 22ms/step - accuracy: 0.9974 - loss: 0.0143\n",
      "Epoch 17/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9965 - loss: 0.0139\n",
      "Epoch 18/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9969 - loss: 0.0137\n",
      "Epoch 19/40\n",
      "225/225 - 5s - 24ms/step - accuracy: 0.9965 - loss: 0.0134\n",
      "Epoch 20/40\n",
      "225/225 - 4s - 19ms/step - accuracy: 0.9971 - loss: 0.0124\n",
      "Epoch 21/40\n",
      "225/225 - 5s - 22ms/step - accuracy: 0.9971 - loss: 0.0108\n",
      "Epoch 22/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9969 - loss: 0.0124\n",
      "Epoch 23/40\n",
      "225/225 - 5s - 22ms/step - accuracy: 0.9968 - loss: 0.0119\n",
      "Epoch 24/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9967 - loss: 0.0114\n",
      "Epoch 25/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9965 - loss: 0.0123\n",
      "Epoch 26/40\n",
      "225/225 - 4s - 19ms/step - accuracy: 0.9971 - loss: 0.0099\n",
      "Epoch 27/40\n",
      "225/225 - 4s - 18ms/step - accuracy: 0.9965 - loss: 0.0118\n",
      "Epoch 28/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9972 - loss: 0.0097\n",
      "Epoch 29/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9965 - loss: 0.0117\n",
      "Epoch 30/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0096\n",
      "Epoch 31/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9967 - loss: 0.0106\n",
      "Epoch 32/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9968 - loss: 0.0096\n",
      "Epoch 33/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9967 - loss: 0.0097\n",
      "Epoch 34/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9968 - loss: 0.0105\n",
      "Epoch 35/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9974 - loss: 0.0090\n",
      "Epoch 36/40\n",
      "225/225 - 4s - 16ms/step - accuracy: 0.9974 - loss: 0.0090\n",
      "Epoch 37/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9967 - loss: 0.0099\n",
      "Epoch 38/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9968 - loss: 0.0102\n",
      "Epoch 39/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9965 - loss: 0.0109\n",
      "Epoch 40/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9971 - loss: 0.0090\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.9978 - loss: 0.0064\n",
      "40/40 - 0s - 3ms/step - accuracy: 0.5917 - loss: 2.0699\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Epoch 1/40\n",
      "225/225 - 4s - 19ms/step - accuracy: 0.6355 - loss: 0.6435\n",
      "Epoch 2/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.6928 - loss: 0.5817\n",
      "Epoch 3/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.7436 - loss: 0.5175\n",
      "Epoch 4/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.8171 - loss: 0.4255\n",
      "Epoch 5/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.8965 - loss: 0.3065\n",
      "Epoch 6/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9547 - loss: 0.1941\n",
      "Epoch 7/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9865 - loss: 0.1146\n",
      "Epoch 8/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9935 - loss: 0.0694\n",
      "Epoch 9/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9961 - loss: 0.0449\n",
      "Epoch 10/40\n",
      "225/225 - 5s - 24ms/step - accuracy: 0.9964 - loss: 0.0327\n",
      "Epoch 11/40\n",
      "225/225 - 5s - 22ms/step - accuracy: 0.9968 - loss: 0.0251\n",
      "Epoch 12/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9972 - loss: 0.0199\n",
      "Epoch 13/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0179\n",
      "Epoch 14/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9968 - loss: 0.0165\n",
      "Epoch 15/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0139\n",
      "Epoch 16/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9974 - loss: 0.0126\n",
      "Epoch 17/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9971 - loss: 0.0131\n",
      "Epoch 18/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0129\n",
      "Epoch 19/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9974 - loss: 0.0104\n",
      "Epoch 20/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9974 - loss: 0.0102\n",
      "Epoch 21/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0111\n",
      "Epoch 22/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0107\n",
      "Epoch 23/40\n",
      "225/225 - 5s - 23ms/step - accuracy: 0.9972 - loss: 0.0095\n",
      "Epoch 24/40\n",
      "225/225 - 5s - 23ms/step - accuracy: 0.9968 - loss: 0.0101\n",
      "Epoch 25/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9971 - loss: 0.0098\n",
      "Epoch 26/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9972 - loss: 0.0093\n",
      "Epoch 27/40\n",
      "225/225 - 3s - 14ms/step - accuracy: 0.9972 - loss: 0.0102\n",
      "Epoch 28/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9969 - loss: 0.0094\n",
      "Epoch 29/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9972 - loss: 0.0091\n",
      "Epoch 30/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9975 - loss: 0.0087\n",
      "Epoch 31/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9971 - loss: 0.0100\n",
      "Epoch 32/40\n",
      "225/225 - 3s - 13ms/step - accuracy: 0.9971 - loss: 0.0080\n",
      "Epoch 33/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9974 - loss: 0.0086\n",
      "Epoch 34/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9972 - loss: 0.0094\n",
      "Epoch 35/40\n",
      "225/225 - 3s - 16ms/step - accuracy: 0.9972 - loss: 0.0084\n",
      "Epoch 36/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9978 - loss: 0.0079\n",
      "Epoch 37/40\n",
      "225/225 - 4s - 16ms/step - accuracy: 0.9971 - loss: 0.0084\n",
      "Epoch 38/40\n",
      "225/225 - 3s - 15ms/step - accuracy: 0.9979 - loss: 0.0065\n",
      "Epoch 39/40\n",
      "225/225 - 5s - 22ms/step - accuracy: 0.9976 - loss: 0.0072\n",
      "Epoch 40/40\n",
      "225/225 - 5s - 23ms/step - accuracy: 0.9978 - loss: 0.0073\n",
      "225/225 - 1s - 3ms/step - accuracy: 0.9981 - loss: 0.0057\n",
      "40/40 - 0s - 3ms/step - accuracy: 0.6003 - loss: 2.0298\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_5880\\426752499.py:53: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    NN results\n",
    "'''\n",
    "\n",
    "EPOCHS = 10\n",
    "nnOutput = {}\n",
    "nnMatrices = {\"tweet_classifier\" : None,\n",
    "               \"imdb_classifier\" : None,\n",
    "               \"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "\n",
    "# tweet NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "tweet_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = tweet_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = tweet_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"tweet_classifier\"] = model_acc2\n",
    "tweet_pred = [1 if i >= 0.5 else 0 for i in tweet_model.predict(x_test)]\n",
    "nnMatrices[\"tweet_classifier\"] = confusion_matrix(y_test, tweet_pred)\n",
    "\n",
    "# imdb NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[imdb_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "imdb_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = imdb_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = imdb_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"imdb_classifier\"] = model_acc2\n",
    "imdb_pred = [1 if i >= 0.5 else 0 for i in imdb_model.predict(x_test)]\n",
    "nnMatrices[\"imdb_classifier\"] = confusion_matrix(y_test, imdb_pred)\n",
    "\n",
    "# combined NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[tweet_and_imdb].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "combined_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = combined_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = combined_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"combined_classifier\"] = model_acc2\n",
    "combined_pred = [1 if i >= 0.5 else 0 for i in combined_model.predict(x_test)]\n",
    "nnMatrices[\"combined_classifier\"] = confusion_matrix(y_test, combined_pred)\n",
    "\n",
    "# raw NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered[raw_features].to_numpy(), classes, test_size=0.15, random_state=42)\n",
    "raw_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"raw_sentiments\"] = model_acc2\n",
    "raw_pred = [1 if i >= 0.5 else 0 for i in raw_model.predict(x_test)]\n",
    "nnMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, raw_pred)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nnMatrices.keys():\n",
    "  pd.DataFrame(nnMatrices[i]).to_excel(writer, sheet_name=f\"matrix_NN_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "accuracy_df.loc[\"Neural Net\"] = nnOutput\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 - 1s - 3ms/step - accuracy: 0.9981 - loss: 0.0057\n",
      "40/40 - 0s - 3ms/step - accuracy: 0.6003 - loss: 2.0298\n",
      "Train / Test Accuracy: 99.8% / 60.0%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tCategorical feature sets for Naive Bayes and Random Forests\n",
    "'''\n",
    "\n",
    "\n",
    "# both sentiment classifiers\n",
    "categorical_tweet_and_imdb = filtered[[\"text_tb_sub_class\",  \"text_NN_imdb\", \"text_NN_tweets\",\t\n",
    "                                       \t \"text_log_imdb\", \"text_log_tweets\",  ]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "categorical_raw_features = filtered[[\"text_vader_class\", \"text_tb_pol_class\", \"text_tb_sub_class\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\801353151.py:65: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.623518         0.623518             0.623518   \n",
       "Neural Net              0.375494         0.623518             0.623518   \n",
       "Naive Bayes             0.000000         0.000000             0.623518   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.623518  \n",
       "Neural Net            0.623518  \n",
       "Naive Bayes           0.623518  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  Naive Bayes work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "nbOutput = {}\n",
    "nbMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "nb = CategoricalNB()\n",
    "\n",
    "nb_combined_pred = None\n",
    "nb_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_combined_pred = nb.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, nb_combined_pred)\n",
    "nbMatrices[\"combined_classifier\"] = confusion_matrix(y_test, nb_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_raw_pred = nb.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, nb_raw_pred)\n",
    "nbMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, nb_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "nbOutput[\"tweet_classifier\"] = 0\n",
    "nbOutput[\"imdb_classifier\"] = 0\n",
    "nbOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "nbOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "nbMatrices[\"combined_classifier\"] = np.mean(np.array(nbMatrices[\"combined_classifier\"]), axis=0)\n",
    "nbMatrices[\"raw_sentiments\"] = np.mean(np.array(nbMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nbMatrices.keys():\n",
    "  pd.DataFrame(nbMatrices[i]).to_excel(writer, sheet_name=f\"matrix_nb_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Naive Bayes\"] = nbOutput\n",
    "\n",
    "accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\3660210348.py:57: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.375494</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.623518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.623518</td>\n",
       "      <td>0.622530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.623518         0.623518             0.623518   \n",
       "Neural Net              0.375494         0.623518             0.623518   \n",
       "Naive Bayes             0.000000         0.000000             0.623518   \n",
       "Random Forest           0.000000         0.000000             0.623518   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.623518  \n",
       "Neural Net            0.623518  \n",
       "Naive Bayes           0.623518  \n",
       "Random Forest         0.622530  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Random forests work\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# average accuracies\n",
    "rfOutput = {}\n",
    "rfMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_combined_pred = None\n",
    "rf_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_combined_pred = rf.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, rf_combined_pred)\n",
    "rfMatrices[\"combined_classifier\"] = confusion_matrix(y_test, rf_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_raw_pred = rf.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, rf_raw_pred)\n",
    "rfMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, rf_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "rfOutput[\"tweet_classifier\"] = 0\n",
    "rfOutput[\"imdb_classifier\"] = 0\n",
    "rfOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "rfOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "rfMatrices[\"combined_classifier\"] = np.mean(np.array(rfMatrices[\"combined_classifier\"]), axis=0)\n",
    "rfMatrices[\"raw_sentiments\"] = np.mean(np.array(rfMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in rfMatrices.keys():\n",
    "  pd.DataFrame(rfMatrices[i]).to_excel(writer, sheet_name=f\"matrix_rf_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Random Forest\"] = rfOutput\n",
    "\n",
    "accuracy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_17008\\3651036480.py:7: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Finally, save accuracy metrics to the spreadsheet\n",
    "'''\n",
    "\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "accuracy_df.to_excel(writer, sheet_name=f\"predicion_accuracies\")\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl.drawing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "y = y_test\n",
    "log_y = log_y_actual\n",
    "\n",
    "raw_scores = np.array(raw_pred)\n",
    "combined_scores = np.array(combined_pred)\n",
    "\n",
    "raw_fpr, raw_tpr, raw_thresh = metrics.roc_curve(y, raw_scores, pos_label=1)\n",
    "raw_roc_auc = metrics.auc(raw_fpr, raw_tpr)\n",
    "\n",
    "com_fpr, com_tpr, com_thresh = metrics.roc_curve(y, combined_scores, pos_label=1)\n",
    "com_roc_auc = metrics.auc(com_fpr, com_tpr)\n",
    "\n",
    "raw_fpr_log, raw_tpr_log, raw_thresh_log = metrics.roc_curve(y, log_raw_pred, pos_label=1)\n",
    "raw_roc_auc_log = metrics.auc(raw_fpr_log, raw_tpr_log)\n",
    "\n",
    "com_fpr_log, com_tpr_log, com_thresh_log = metrics.roc_curve(y, log_combined_pred, pos_label=1)\n",
    "com_roc_auc_log = metrics.auc(com_fpr_log, com_tpr_log)\n",
    "\n",
    "raw_fpr_nb, raw_tpr_nb, raw_thresh_nb = metrics.roc_curve(y, nb_raw_pred, pos_label=1)\n",
    "raw_roc_auc_nb = metrics.auc(raw_fpr_nb, raw_tpr_nb)\n",
    "\n",
    "com_fpr_nb, com_tpr_nb, com_thresh_nb = metrics.roc_curve(y, nb_combined_pred, pos_label=1)\n",
    "com_roc_auc_nb = metrics.auc(com_fpr_nb, com_tpr_nb)\n",
    "\n",
    "raw_fpr_rf, raw_tpr_rf, raw_thresh_rf = metrics.roc_curve(y, rf_raw_pred, pos_label=1)\n",
    "raw_roc_auc_rf = metrics.auc(raw_fpr_rf, raw_tpr_rf)\n",
    "\n",
    "com_fpr_rf, com_tpr_rf, com_thresh_rf = metrics.roc_curve(y, rf_combined_pred, pos_label=1)\n",
    "com_roc_auc_rf = metrics.auc(com_fpr_rf, com_tpr_rf)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(raw_fpr, raw_tpr,\n",
    " lw=lw, label='Raw NN (%0.2f)' % raw_roc_auc)\n",
    "plt.plot(com_fpr, com_tpr,\n",
    " lw=lw, label='Sentiment Classifier NN  (%0.2f)' % com_roc_auc)\n",
    "plt.plot(raw_fpr_log, raw_tpr_log,\n",
    " lw=lw, label='Raw Log (%0.2f)' % raw_roc_auc_log)\n",
    "plt.plot(com_fpr_log, com_tpr_log,\n",
    " lw=lw, label='Sentiment Classifier Log  (%0.2f)' % com_roc_auc_log)\n",
    "\n",
    "plt.plot(raw_fpr_nb, raw_tpr_nb,\n",
    " lw=lw, label='Raw Naive Bayes (%0.2f)' % raw_roc_auc_nb)\n",
    "plt.plot(com_fpr_nb, com_tpr_nb,\n",
    " lw=lw, label='Sentiment Classifier Naive Bayes  (%0.2f)' % com_roc_auc_nb)\n",
    "\n",
    "plt.plot(raw_fpr_rf, raw_tpr_rf,\n",
    " lw=lw, label='Raw Naive Random Forest (%0.2f)' % raw_roc_auc_rf)\n",
    "plt.plot(com_fpr_rf, com_tpr_rf,\n",
    " lw=lw, label='Sentiment Classifier Random Forest  (%0.2f)' % com_roc_auc_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# save figure as PNG\n",
    "png = io.BytesIO()\n",
    "plt.savefig(png, format=\"png\")\n",
    "\n",
    "\n",
    "# write PNG to excel file\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "ws = book.active\n",
    "\n",
    "img = openpyxl.drawing.image.Image(png)\n",
    "img.anchor = \"A1\"\n",
    "ws.add_image(img)\n",
    "book.save(filename=EXCEL_FILE)\n",
    "plt.close()\n",
    "book.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
