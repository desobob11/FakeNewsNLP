{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data = pd.read_csv(r\"../../data/SentimentAnalysis/imdb.csv\", encoding=\"utf8\")\n",
    "\n",
    "data = data.sample(5000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Natural language preprocessing\n",
    "\n",
    "    Remove punctuation, make all words lowercase, and lemmatize\n",
    "'''\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import Word\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import string\n",
    "\n",
    "'''\n",
    "    NLTK has a model to tag words as adjectives, nouns, etc,\n",
    "    but NLTK uses wordnet for lemmatization. wordnet only uses\n",
    "    four possible tags, while NLTK returns tons of unique ones\n",
    "\n",
    "    This function transforms NLTK tags to wordnet tags for lemmatization\n",
    "'''\n",
    "def nltk_tag_to_wordnet(tag: str) -> str:\n",
    "    if tag[0] == \"J\":\n",
    "        return wordnet.ADJ\n",
    "    elif tag[0] == \"V\":\n",
    "        return wordnet.VERB\n",
    "    elif tag[0] == \"N\":\n",
    "        return wordnet.NOUN\n",
    "    elif tag[0] == \"R\":\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "'''\n",
    "    Remove non-alphabetical characters and punctuation\n",
    "'''\n",
    "def keep_only_alphabetic(s: str) -> str:\n",
    "    temp = \"\".join([\" \" if i in string.punctuation else i for i in s])\n",
    "    return \"\".join([i for i in temp if (ord(i) <= 90 and ord(i) >= 65) or (ord(i) <= 122 and ord(i) >= 97) or i.isspace()])\n",
    "\n",
    "'''\n",
    "    Take a string of text, tokenize it, and return a list of lemmatized tokens\n",
    "'''\n",
    "def lemmatize_words(s: str) -> list[str]:\n",
    "    lemmer = nltk.stem.WordNetLemmatizer()\n",
    "    words = [i.lower() for i in word_tokenize(s)]       # tokenize and lowercase\n",
    "    words = [i for i in words if i not in stopwords.words(\"english\")]   # remove stopwords\n",
    "    words = list(filter(lambda x: nltk_tag_to_wordnet(x[1]) !=\"\", pos_tag(words)))  # remove invalid lemmatization words and tags\n",
    "    words = [lemmer.lemmatize(i[0], nltk_tag_to_wordnet(i[1])) for i in words]  #  lemmatize words\n",
    "    return words\n",
    "\n",
    "'''\n",
    "    Combine all functions above to pre-process strng\n",
    "'''\n",
    "def pre_process_text(text: str) -> str:\n",
    "    s = keep_only_alphabetic(text)\n",
    "    lemmatized = lemmatize_words(s)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>For the sake of propaganda during World War II...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17688</th>\n",
       "      <td>The world of the  sci fi drama SOYLENT GREEN i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38185</th>\n",
       "      <td>So ya think you ve seen every Mafia movie ever...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>Such great actors such a disappointment  Marlo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>French horror cinema has seen something of a r...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30172</th>\n",
       "      <td>I s a big struggle  As a story that is surreal...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37237</th>\n",
       "      <td>I m writing this note as a chess player as wel...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>I LOVED this movie  Not as great as   First Da...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>This satire is just really  really dead on  an...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689</th>\n",
       "      <td>I had no expectations other than to be enterta...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "5247   For the sake of propaganda during World War II...  positive\n",
       "17688  The world of the  sci fi drama SOYLENT GREEN i...  positive\n",
       "38185  So ya think you ve seen every Mafia movie ever...  positive\n",
       "11829  Such great actors such a disappointment  Marlo...  negative\n",
       "13799  French horror cinema has seen something of a r...  positive\n",
       "...                                                  ...       ...\n",
       "30172  I s a big struggle  As a story that is surreal...  negative\n",
       "37237  I m writing this note as a chess player as wel...  negative\n",
       "28564  I LOVED this movie  Not as great as   First Da...  positive\n",
       "3535   This satire is just really  really dead on  an...  positive\n",
       "13689  I had no expectations other than to be enterta...  negative\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"] = data.apply(lambda x: keep_only_alphabetic(x[\"text\"]), axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class(s):\n",
    "    match s:\n",
    "        case \"positive\":\n",
    "            return 1\n",
    "        case \"negative\":\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>For the sake of propaganda during World War II...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17688</th>\n",
       "      <td>The world of the  sci fi drama SOYLENT GREEN i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38185</th>\n",
       "      <td>So ya think you ve seen every Mafia movie ever...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>Such great actors such a disappointment  Marlo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>French horror cinema has seen something of a r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30172</th>\n",
       "      <td>I s a big struggle  As a story that is surreal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37237</th>\n",
       "      <td>I m writing this note as a chess player as wel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>I LOVED this movie  Not as great as   First Da...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>This satire is just really  really dead on  an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689</th>\n",
       "      <td>I had no expectations other than to be enterta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "5247   For the sake of propaganda during World War II...          1\n",
       "17688  The world of the  sci fi drama SOYLENT GREEN i...          1\n",
       "38185  So ya think you ve seen every Mafia movie ever...          1\n",
       "11829  Such great actors such a disappointment  Marlo...          0\n",
       "13799  French horror cinema has seen something of a r...          1\n",
       "...                                                  ...        ...\n",
       "30172  I s a big struggle  As a story that is surreal...          0\n",
       "37237  I m writing this note as a chess player as wel...          0\n",
       "28564  I LOVED this movie  Not as great as   First Da...          1\n",
       "3535   This satire is just really  really dead on  an...          1\n",
       "13689  I had no expectations other than to be enterta...          0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"] = data.apply(lambda x: to_class(x[\"sentiment\"]), axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data[\"text_tb_pol\"] = data.apply(lambda x: TextBlob(x[\"text\"]).polarity, axis=1)\n",
    "data[\"text_tb_sub\"] = data.apply(lambda x: TextBlob(x[\"text\"]).subjectivity, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_tb_pol</th>\n",
       "      <th>text_tb_sub</th>\n",
       "      <th>text_vader_comp</th>\n",
       "      <th>text_vader_neg</th>\n",
       "      <th>text_vader_neu</th>\n",
       "      <th>text_vader_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>For the sake of propaganda during World War II...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081061</td>\n",
       "      <td>0.442424</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.806</td>\n",
       "      <td>0.109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17688</th>\n",
       "      <td>The world of the  sci fi drama SOYLENT GREEN i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.070271</td>\n",
       "      <td>0.459549</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38185</th>\n",
       "      <td>So ya think you ve seen every Mafia movie ever...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.300321</td>\n",
       "      <td>0.592308</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.857</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>Such great actors such a disappointment  Marlo...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.247619</td>\n",
       "      <td>0.724921</td>\n",
       "      <td>-0.8365</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13799</th>\n",
       "      <td>French horror cinema has seen something of a r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.167388</td>\n",
       "      <td>0.527629</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30172</th>\n",
       "      <td>I s a big struggle  As a story that is surreal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240353</td>\n",
       "      <td>0.475831</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37237</th>\n",
       "      <td>I m writing this note as a chess player as wel...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043812</td>\n",
       "      <td>0.402317</td>\n",
       "      <td>0.9559</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28564</th>\n",
       "      <td>I LOVED this movie  Not as great as   First Da...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.317500</td>\n",
       "      <td>0.628333</td>\n",
       "      <td>0.9921</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3535</th>\n",
       "      <td>This satire is just really  really dead on  an...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093210</td>\n",
       "      <td>0.607407</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13689</th>\n",
       "      <td>I had no expectations other than to be enterta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.429881</td>\n",
       "      <td>0.521310</td>\n",
       "      <td>0.9545</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment  \\\n",
       "5247   For the sake of propaganda during World War II...          1   \n",
       "17688  The world of the  sci fi drama SOYLENT GREEN i...          1   \n",
       "38185  So ya think you ve seen every Mafia movie ever...          1   \n",
       "11829  Such great actors such a disappointment  Marlo...          0   \n",
       "13799  French horror cinema has seen something of a r...          1   \n",
       "...                                                  ...        ...   \n",
       "30172  I s a big struggle  As a story that is surreal...          0   \n",
       "37237  I m writing this note as a chess player as wel...          0   \n",
       "28564  I LOVED this movie  Not as great as   First Da...          1   \n",
       "3535   This satire is just really  really dead on  an...          1   \n",
       "13689  I had no expectations other than to be enterta...          0   \n",
       "\n",
       "       text_tb_pol  text_tb_sub  text_vader_comp  text_vader_neg  \\\n",
       "5247      0.081061     0.442424           0.8837           0.086   \n",
       "17688     0.070271     0.459549           0.9893           0.061   \n",
       "38185     0.300321     0.592308           0.8625           0.044   \n",
       "11829    -0.247619     0.724921          -0.8365           0.184   \n",
       "13799     0.167388     0.527629           0.9761           0.138   \n",
       "...            ...          ...              ...             ...   \n",
       "30172     0.240353     0.475831           0.9924           0.029   \n",
       "37237     0.043812     0.402317           0.9559           0.083   \n",
       "28564     0.317500     0.628333           0.9921           0.111   \n",
       "3535      0.093210     0.607407           0.9841           0.080   \n",
       "13689     0.429881     0.521310           0.9545           0.045   \n",
       "\n",
       "       text_vader_neu  text_vader_pos  \n",
       "5247            0.806           0.109  \n",
       "17688           0.812           0.128  \n",
       "38185           0.857           0.100  \n",
       "11829           0.719           0.097  \n",
       "13799           0.683           0.179  \n",
       "...               ...             ...  \n",
       "30172           0.759           0.211  \n",
       "37237           0.813           0.104  \n",
       "28564           0.546           0.343  \n",
       "3535            0.725           0.196  \n",
       "13689           0.770           0.185  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "data[\"text_vader_scores\"] = data.apply(lambda x: analyzer.polarity_scores(x[\"text\"]), axis=1)\n",
    "data[\"text_vader_comp\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"compound\"], axis=1)\n",
    "data[\"text_vader_neg\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"neg\"], axis=1)\n",
    "data[\"text_vader_neu\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"neu\"], axis=1)\n",
    "data[\"text_vader_pos\"] = data.apply(lambda x: x[\"text_vader_scores\"][\"pos\"], axis=1)\n",
    "data = data.drop([\"text_vader_scores\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Add tf-idf vectorizer\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "def tf_idf_vectorize(df: pd.DataFrame, corpus: pd.Series, vocabulary: list[str]) -> tuple[list[str], pd.DataFrame]:\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"ascii\", lowercase=True, stop_words=stop_words, max_features=500, ngram_range=(1,3), vocabulary=vocabulary)\n",
    "    features = vectorizer.fit_transform(corpus).toarray()\n",
    "    names = vectorizer.get_feature_names_out()\n",
    "    headers = [f\"__word{i}\" for i in range(len(names))]\n",
    "    feature_frame = pd.DataFrame(features, columns=headers)\n",
    "    #final = pd.concat([df, feature_frame], axis=1)\n",
    "    return (names, feature_frame)\n",
    "\n",
    "\n",
    "def get_full_vocabulary(corpus: pd.Series) -> list[str]:\n",
    "    vectorizer = TfidfVectorizer(strip_accents=\"ascii\", lowercase=True, stop_words=stopwords.words(\"english\"), max_features=500, ngram_range=(1,3))\n",
    "    vectorizer.fit_transform(corpus).toarray()\n",
    "    return vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "def add_tf_idf_vector(x: pd.DataFrame, partial_corpus: pd.Series, full_corpus: pd.Series) -> pd.DataFrame:\n",
    "    temp = pd.concat((x, partial_corpus), axis=1).reset_index()\n",
    "    # first, we need a vocabulary from the entire dataset\n",
    "    vocabulary = get_full_vocabulary(full_corpus)\n",
    "\n",
    "    tf_idf_vector = tf_idf_vectorize(temp, temp[partial_corpus.name], vocabulary=vocabulary)[1].reset_index()\n",
    "    temp = temp.reset_index()\n",
    "    temp = pd.concat((temp, tf_idf_vector), axis=1).drop([partial_corpus.name, \"index\"], axis=1)\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "1\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "2\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "3\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "4\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "Test accuracy on 5-fold cross-validation: 0.8356\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    POLARITY + TF-IDF\n",
    "'''\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "classes = data[\"sentiment\"]\n",
    "features = data[[\"text_tb_pol\",  \"text_vader_pos\", \"text_vader_neg\", \"text_vader_neu\"]]\n",
    "\n",
    "\n",
    "\n",
    "test_total = 0\n",
    "run_count = 0\n",
    "\n",
    "\n",
    "#lbgfs\n",
    "x = features\n",
    "y = classes\n",
    "folds = 5\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    print(i)\n",
    "    scaler = StandardScaler()\n",
    "    print(\"tf_idf training\")\n",
    "    x_train = add_tf_idf_vector(x.iloc[train_index], data[\"text\"].iloc[train_index], data[\"text\"])\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "    print(\"tf_idf testing\")\n",
    "    x_test = add_tf_idf_vector(x.iloc[test_index], data[\"text\"].iloc[test_index], data[\"text\"])\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(max_iter=500)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ], voting='soft')\n",
    "    print(\"ensemble fitting\")\n",
    "    ensemble.fit(x_train, y_train)\n",
    "    y_pred = ensemble.predict(x_test)\n",
    "    test_total += accuracy_score(y_test, y_pred)\n",
    "    run_count += 1\n",
    "\n",
    "print(f\"Test accuracy on {folds}-fold cross-validation: {test_total / run_count}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ensemble fitting\n",
      "1\n",
      "ensemble fitting\n",
      "2\n",
      "ensemble fitting\n",
      "3\n",
      "ensemble fitting\n",
      "4\n",
      "ensemble fitting\n",
      "Test accuracy on 5-fold cross-validation: 0.7780000000000001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    POLARITY ONLY\n",
    "'''\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "classes = data[\"sentiment\"]\n",
    "features = data[[\"text_tb_pol\",  \"text_vader_pos\", \"text_vader_neg\", \"text_vader_neu\"]]\n",
    "\n",
    "\n",
    "\n",
    "test_total = 0\n",
    "run_count = 0\n",
    "\n",
    "\n",
    "#lbgfs\n",
    "x = features\n",
    "y = classes\n",
    "folds = 5\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    print(i)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    x_train = x.iloc[train_index]\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "\n",
    "    x_test = x.iloc[test_index]\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(max_iter=500)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ], voting='soft')\n",
    "    print(\"ensemble fitting\")\n",
    "    ensemble.fit(x_train, y_train)\n",
    "    y_pred = ensemble.predict(x_test)\n",
    "    test_total += accuracy_score(y_test, y_pred)\n",
    "    run_count += 1\n",
    "\n",
    "print(f\"Test accuracy on {folds}-fold cross-validation: {test_total / run_count}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "1\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "2\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "3\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "4\n",
      "tf_idf training\n",
      "tf_idf testing\n",
      "ensemble fitting\n",
      "Test accuracy on 5-fold cross-validation: 0.8242\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    TF-IDF ONLY\n",
    "'''\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "classes = data[\"sentiment\"]\n",
    "features = data[[\"text_tb_pol\"]]\n",
    "\n",
    "\n",
    "\n",
    "test_total = 0\n",
    "run_count = 0\n",
    "\n",
    "\n",
    "#lbgfs\n",
    "x = features\n",
    "y = classes\n",
    "folds = 5\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(x)):\n",
    "    print(i)\n",
    "    scaler = StandardScaler()\n",
    "    print(\"tf_idf training\")\n",
    "    x_train = add_tf_idf_vector(x.iloc[train_index], data[\"text\"].iloc[train_index], data[\"text\"]).drop(\"text_tb_pol\", axis=1)\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "    print(\"tf_idf testing\")\n",
    "    x_test = add_tf_idf_vector(x.iloc[test_index], data[\"text\"].iloc[test_index], data[\"text\"]).drop(\"text_tb_pol\", axis=1)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_train = y.iloc[train_index]\n",
    "    y_test = y.iloc[test_index]\n",
    "\n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(max_iter=500)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ], voting='soft')\n",
    "    print(\"ensemble fitting\")\n",
    "    ensemble.fit(x_train, y_train)\n",
    "    y_pred = ensemble.predict(x_test)\n",
    "    test_total += accuracy_score(y_test, y_pred)\n",
    "    run_count += 1\n",
    "\n",
    "print(f\"Test accuracy on {folds}-fold cross-validation: {test_total / run_count}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"IMDB_ENSEMBLE_.pkl\", \"wb\") as file:\n",
    "    pickle.dump(ensemble, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
