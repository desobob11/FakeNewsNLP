{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    ISOT\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/ISOT_PROCESSED_2.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "'''\n",
    "    Define filtered dataset, classes, features, dataframe for model accuracies, and excel file for results\n",
    "'''\n",
    "\n",
    "'''Drop NA'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"flag\"].to_numpy()\n",
    "\n",
    "#just tweet sentiment classifier\n",
    "tweet_features = filtered[[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\"]].to_numpy()\n",
    "\n",
    "#just imbd sentiment classifier\n",
    "imdb_features = filtered[[\"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]].to_numpy()\n",
    "\n",
    "# both sentiment classifiers\n",
    "tweet_and_imdb = filtered[[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\",\n",
    "                           \"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "raw_features = filtered[[\"text_tb_pol\",\t\"text_tb_sub\",\t\"title_tb_pol\",\t\"title_tb_sub\",\t\"title_vader_comp\",\t\"title_vader_neg\",\t\n",
    "              \"title_vader_neu\",\t\"title_vader_pos\",\t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "# dataframe to store accuracies for NN and log regression\n",
    "accuracy_df = pd.DataFrame(columns=[\"tweet_classifier\",\n",
    "               \"imdb_classifier\",\n",
    "               \"combined_classifier\",\n",
    "               \"raw_sentiments\"])\n",
    "\n",
    "\n",
    "EXCEL_FILE = r\"../data/Des_fake_news/Sentiment_Analysis_Results/ISOT_RESULTS.xlsx\"\n",
    "# overwrite book if exists\n",
    "book = Workbook()\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_7372\\2003894392.py:78: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Logistic Regression work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "logOutput = {}\n",
    "logMatrices = {\"tweet_classifier\" : [],\n",
    "               \"imdb_classifier\" : [],\n",
    "               \"combined_classifier\": [],\n",
    "               \"raw_sentiments\" : []}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "log_combined_pred = None\n",
    "log_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "tweet_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"tweet_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "imdb_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"imdb_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_combined_pred = lbgfs.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, log_combined_pred)\n",
    "logMatrices[\"combined_classifier\"].append(confusion_matrix(y_test, log_combined_pred))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_raw_pred = lbgfs.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, log_raw_pred)\n",
    "logMatrices[\"raw_sentiments\"].append(confusion_matrix(y_test, log_raw_pred))\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "logOutput[\"tweet_classifier\"] = tweet_scores / 1\n",
    "logOutput[\"imdb_classifier\"] = imdb_scores / 1\n",
    "logOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "logOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "logMatrices[\"tweet_classifier\"] = np.mean(np.array(logMatrices[\"tweet_classifier\"]), axis=0)\n",
    "logMatrices[\"imdb_classifier\"] = np.mean(np.array(logMatrices[\"imdb_classifier\"]), axis=0)\n",
    "logMatrices[\"combined_classifier\"] = np.mean(np.array(logMatrices[\"combined_classifier\"]), axis=0)\n",
    "logMatrices[\"raw_sentiments\"] = np.mean(np.array(logMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in logMatrices.keys():\n",
    "  pd.DataFrame(logMatrices[i]).to_excel(writer, sheet_name=f\"matrix_log_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Log Regression\"] = logOutput\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and save neural net models\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tweet_len = tweet_features.shape[1]\n",
    "imdb_len = imdb_features.shape[1]\n",
    "combined_len = tweet_and_imdb.shape[1]\n",
    "raw_len = raw_features.shape[1]\n",
    "\n",
    "\n",
    "tweet_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(tweet_len, 1)),\n",
    "  tf.keras.layers.Dense(tweet_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "tweet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "imdb_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(imdb_len, 1)),\n",
    "  tf.keras.layers.Dense(imdb_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "imdb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "combined_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(combined_len, 1)),\n",
    "  tf.keras.layers.Dense(combined_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "raw_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(raw_len, 1)),\n",
    "  tf.keras.layers.Dense(raw_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "raw_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 - 3s - 3ms/step - accuracy: 0.6273 - loss: 0.6548\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6533 - loss: 0.6206\n",
      "211/211 - 0s - 943us/step - accuracy: 0.6485 - loss: 0.6229\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "1192/1192 - 3s - 2ms/step - accuracy: 0.5200 - loss: 0.6919\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.5249 - loss: 0.6915\n",
      "211/211 - 0s - 939us/step - accuracy: 0.5225 - loss: 0.6919\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6236 - loss: 0.6448\n",
      "Epoch 2/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6567 - loss: 0.6165\n",
      "Epoch 3/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6587 - loss: 0.6136\n",
      "Epoch 4/40\n",
      "1192/1192 - 1s - 980us/step - accuracy: 0.6599 - loss: 0.6125\n",
      "Epoch 5/40\n",
      "1192/1192 - 1s - 962us/step - accuracy: 0.6594 - loss: 0.6120\n",
      "Epoch 6/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6603 - loss: 0.6118\n",
      "Epoch 7/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6598 - loss: 0.6117\n",
      "Epoch 8/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.6593 - loss: 0.6116\n",
      "Epoch 9/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6588 - loss: 0.6114\n",
      "Epoch 10/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6597 - loss: 0.6114\n",
      "Epoch 11/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6612 - loss: 0.6111\n",
      "Epoch 12/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6607 - loss: 0.6112\n",
      "Epoch 13/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6611 - loss: 0.6112\n",
      "Epoch 14/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.6601 - loss: 0.6111\n",
      "Epoch 15/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6617 - loss: 0.6110\n",
      "Epoch 16/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6627 - loss: 0.6109\n",
      "Epoch 17/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6614 - loss: 0.6110\n",
      "Epoch 18/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6622 - loss: 0.6109\n",
      "Epoch 19/40\n",
      "1192/1192 - 1s - 963us/step - accuracy: 0.6593 - loss: 0.6109\n",
      "Epoch 20/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6608 - loss: 0.6109\n",
      "Epoch 21/40\n",
      "1192/1192 - 1s - 970us/step - accuracy: 0.6619 - loss: 0.6108\n",
      "Epoch 22/40\n",
      "1192/1192 - 1s - 976us/step - accuracy: 0.6612 - loss: 0.6109\n",
      "Epoch 23/40\n",
      "1192/1192 - 1s - 977us/step - accuracy: 0.6620 - loss: 0.6108\n",
      "Epoch 24/40\n",
      "1192/1192 - 1s - 974us/step - accuracy: 0.6613 - loss: 0.6107\n",
      "Epoch 25/40\n",
      "1192/1192 - 1s - 998us/step - accuracy: 0.6611 - loss: 0.6106\n",
      "Epoch 26/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6619 - loss: 0.6107\n",
      "Epoch 27/40\n",
      "1192/1192 - 1s - 980us/step - accuracy: 0.6628 - loss: 0.6107\n",
      "Epoch 28/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6623 - loss: 0.6108\n",
      "Epoch 29/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6617 - loss: 0.6106\n",
      "Epoch 30/40\n",
      "1192/1192 - 1s - 993us/step - accuracy: 0.6608 - loss: 0.6108\n",
      "Epoch 31/40\n",
      "1192/1192 - 1s - 988us/step - accuracy: 0.6602 - loss: 0.6106\n",
      "Epoch 32/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6613 - loss: 0.6105\n",
      "Epoch 33/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6612 - loss: 0.6108\n",
      "Epoch 34/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6611 - loss: 0.6106\n",
      "Epoch 35/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6617 - loss: 0.6107\n",
      "Epoch 36/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.6619 - loss: 0.6104\n",
      "Epoch 37/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.6616 - loss: 0.6106\n",
      "Epoch 38/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6619 - loss: 0.6107\n",
      "Epoch 39/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6610 - loss: 0.6106\n",
      "Epoch 40/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6617 - loss: 0.6105\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6623 - loss: 0.6125\n",
      "211/211 - 0s - 981us/step - accuracy: 0.6515 - loss: 0.6176\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6564 - loss: 0.6206\n",
      "Epoch 2/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7046 - loss: 0.5778\n",
      "Epoch 3/40\n",
      "1192/1192 - 1s - 962us/step - accuracy: 0.7241 - loss: 0.5577\n",
      "Epoch 4/40\n",
      "1192/1192 - 1s - 976us/step - accuracy: 0.7363 - loss: 0.5422\n",
      "Epoch 5/40\n",
      "1192/1192 - 1s - 983us/step - accuracy: 0.7459 - loss: 0.5291\n",
      "Epoch 6/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7532 - loss: 0.5176\n",
      "Epoch 7/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7573 - loss: 0.5085\n",
      "Epoch 8/40\n",
      "1192/1192 - 1s - 968us/step - accuracy: 0.7631 - loss: 0.5006\n",
      "Epoch 9/40\n",
      "1192/1192 - 1s - 970us/step - accuracy: 0.7644 - loss: 0.4943\n",
      "Epoch 10/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7671 - loss: 0.4891\n",
      "Epoch 11/40\n",
      "1192/1192 - 1s - 988us/step - accuracy: 0.7697 - loss: 0.4856\n",
      "Epoch 12/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7698 - loss: 0.4825\n",
      "Epoch 13/40\n",
      "1192/1192 - 1s - 953us/step - accuracy: 0.7718 - loss: 0.4799\n",
      "Epoch 14/40\n",
      "1192/1192 - 1s - 985us/step - accuracy: 0.7711 - loss: 0.4782\n",
      "Epoch 15/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7717 - loss: 0.4764\n",
      "Epoch 16/40\n",
      "1192/1192 - 1s - 953us/step - accuracy: 0.7724 - loss: 0.4754\n",
      "Epoch 17/40\n",
      "1192/1192 - 1s - 977us/step - accuracy: 0.7731 - loss: 0.4740\n",
      "Epoch 18/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7733 - loss: 0.4727\n",
      "Epoch 19/40\n",
      "1192/1192 - 1s - 937us/step - accuracy: 0.7738 - loss: 0.4718\n",
      "Epoch 20/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7733 - loss: 0.4711\n",
      "Epoch 21/40\n",
      "1192/1192 - 1s - 952us/step - accuracy: 0.7752 - loss: 0.4703\n",
      "Epoch 22/40\n",
      "1192/1192 - 1s - 956us/step - accuracy: 0.7743 - loss: 0.4697\n",
      "Epoch 23/40\n",
      "1192/1192 - 1s - 997us/step - accuracy: 0.7748 - loss: 0.4689\n",
      "Epoch 24/40\n",
      "1192/1192 - 1s - 951us/step - accuracy: 0.7757 - loss: 0.4683\n",
      "Epoch 25/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7744 - loss: 0.4678\n",
      "Epoch 26/40\n",
      "1192/1192 - 1s - 985us/step - accuracy: 0.7750 - loss: 0.4670\n",
      "Epoch 27/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7770 - loss: 0.4665\n",
      "Epoch 28/40\n",
      "1192/1192 - 1s - 948us/step - accuracy: 0.7762 - loss: 0.4662\n",
      "Epoch 29/40\n",
      "1192/1192 - 1s - 979us/step - accuracy: 0.7765 - loss: 0.4657\n",
      "Epoch 30/40\n",
      "1192/1192 - 1s - 969us/step - accuracy: 0.7761 - loss: 0.4651\n",
      "Epoch 31/40\n",
      "1192/1192 - 1s - 967us/step - accuracy: 0.7763 - loss: 0.4650\n",
      "Epoch 32/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7762 - loss: 0.4646\n",
      "Epoch 33/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7762 - loss: 0.4643\n",
      "Epoch 34/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7758 - loss: 0.4644\n",
      "Epoch 35/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7769 - loss: 0.4637\n",
      "Epoch 36/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7771 - loss: 0.4634\n",
      "Epoch 37/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7770 - loss: 0.4634\n",
      "Epoch 38/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7760 - loss: 0.4633\n",
      "Epoch 39/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7771 - loss: 0.4627\n",
      "Epoch 40/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7771 - loss: 0.4626\n",
      "1192/1192 - 1s - 961us/step - accuracy: 0.7778 - loss: 0.4614\n",
      "211/211 - 0s - 976us/step - accuracy: 0.7774 - loss: 0.4629\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_7372\\1142495627.py:53: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    NN results\n",
    "'''\n",
    "\n",
    "EPOCHS = 40\n",
    "nnOutput = {}\n",
    "nnMatrices = {\"tweet_classifier\" : None,\n",
    "               \"imdb_classifier\" : None,\n",
    "               \"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "\n",
    "# tweet NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "tweet_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = tweet_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = tweet_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"tweet_classifier\"] = model_acc2\n",
    "tweet_pred = [1 if i >= 0.5 else 0 for i in tweet_model.predict(x_test)]\n",
    "nnMatrices[\"tweet_classifier\"] = confusion_matrix(y_test, tweet_pred)\n",
    "\n",
    "# imdb NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=42)\n",
    "imdb_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = imdb_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = imdb_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"imdb_classifier\"] = model_acc2\n",
    "imdb_pred = [1 if i >= 0.5 else 0 for i in imdb_model.predict(x_test)]\n",
    "nnMatrices[\"imdb_classifier\"] = confusion_matrix(y_test, imdb_pred)\n",
    "\n",
    "# combined NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "combined_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = combined_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = combined_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"combined_classifier\"] = model_acc2\n",
    "combined_pred = [1 if i >= 0.5 else 0 for i in combined_model.predict(x_test)]\n",
    "nnMatrices[\"combined_classifier\"] = confusion_matrix(y_test, combined_pred)\n",
    "\n",
    "# raw NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "raw_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"raw_sentiments\"] = model_acc2\n",
    "raw_pred = [1 if i >= 0.5 else 0 for i in raw_model.predict(x_test)]\n",
    "nnMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, raw_pred)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nnMatrices.keys():\n",
    "  pd.DataFrame(nnMatrices[i]).to_excel(writer, sheet_name=f\"matrix_NN_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "accuracy_df.loc[\"Neural Net\"] = nnOutput\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\tCategorical feature sets for Naive Bayes and Random Forests\n",
    "'''\n",
    "\n",
    "\n",
    "# both sentiment classifiers\n",
    "categorical_tweet_and_imdb = filtered[[\"text_tb_sub_class\", \"title_tb_sub_class\", \"text_NN_imdb\", \"text_NN_tweets\",\t\"title_NN_imdb\",\n",
    "                                       \t\"title_NN_tweets\", \"text_log_imdb\", \"text_log_tweets\", \"title_log_imdb\", \"title_log_tweets\"]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "categorical_raw_features = filtered[[\"title_vader_class\", \"text_vader_class\", \"text_tb_pol_class\", \"text_tb_sub_class\", \"title_tb_pol_class\", \"title_tb_sub_class\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_7372\\801353151.py:65: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.523845</td>\n",
       "      <td>0.638687</td>\n",
       "      <td>0.711930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.648492</td>\n",
       "      <td>0.522508</td>\n",
       "      <td>0.651463</td>\n",
       "      <td>0.777448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.662903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.638092         0.523845             0.638687   \n",
       "Neural Net              0.648492         0.522508             0.651463   \n",
       "Naive Bayes             0.000000         0.000000             0.638092   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.711930  \n",
       "Neural Net            0.777448  \n",
       "Naive Bayes           0.662903  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  Naive Bayes work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "nbOutput = {}\n",
    "nbMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "nb = CategoricalNB()\n",
    "\n",
    "nb_combined_pred = None\n",
    "nb_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_combined_pred = nb.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, nb_combined_pred)\n",
    "nbMatrices[\"combined_classifier\"] = confusion_matrix(y_test, nb_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "nb.fit(x_train, y_train)\n",
    "nb_raw_pred = nb.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, nb_raw_pred)\n",
    "nbMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, nb_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "nbOutput[\"tweet_classifier\"] = 0\n",
    "nbOutput[\"imdb_classifier\"] = 0\n",
    "nbOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "nbOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "nbMatrices[\"combined_classifier\"] = np.mean(np.array(nbMatrices[\"combined_classifier\"]), axis=0)\n",
    "nbMatrices[\"raw_sentiments\"] = np.mean(np.array(nbMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nbMatrices.keys():\n",
    "  pd.DataFrame(nbMatrices[i]).to_excel(writer, sheet_name=f\"matrix_nb_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Naive Bayes\"] = nbOutput\n",
    "\n",
    "accuracy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_7372\\3660210348.py:57: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.523845</td>\n",
       "      <td>0.638687</td>\n",
       "      <td>0.711930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Net</th>\n",
       "      <td>0.648492</td>\n",
       "      <td>0.522508</td>\n",
       "      <td>0.651463</td>\n",
       "      <td>0.777448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Naive Bayes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.662903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.654138</td>\n",
       "      <td>0.663646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.638092         0.523845             0.638687   \n",
       "Neural Net              0.648492         0.522508             0.651463   \n",
       "Naive Bayes             0.000000         0.000000             0.638092   \n",
       "Random Forest           0.000000         0.000000             0.654138   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.711930  \n",
       "Neural Net            0.777448  \n",
       "Naive Bayes           0.662903  \n",
       "Random Forest         0.663646  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Random forests work\n",
    "'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# average accuracies\n",
    "rfOutput = {}\n",
    "rfMatrices = {\"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf_combined_pred = None\n",
    "rf_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_combined_pred = rf.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, rf_combined_pred)\n",
    "rfMatrices[\"combined_classifier\"] = confusion_matrix(y_test, rf_combined_pred)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(categorical_raw_features, classes, test_size=0.15, random_state=42)\n",
    "rf.fit(x_train, y_train)\n",
    "rf_raw_pred = rf.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, rf_raw_pred)\n",
    "rfMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, rf_raw_pred)\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "rfOutput[\"tweet_classifier\"] = 0\n",
    "rfOutput[\"imdb_classifier\"] = 0\n",
    "rfOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "rfOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "\n",
    "rfMatrices[\"combined_classifier\"] = np.mean(np.array(rfMatrices[\"combined_classifier\"]), axis=0)\n",
    "rfMatrices[\"raw_sentiments\"] = np.mean(np.array(rfMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in rfMatrices.keys():\n",
    "  pd.DataFrame(rfMatrices[i]).to_excel(writer, sheet_name=f\"matrix_rf_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Random Forest\"] = rfOutput\n",
    "\n",
    "accuracy_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_7372\\3651036480.py:7: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Finally, save accuracy metrics to the spreadsheet\n",
    "'''\n",
    "\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "accuracy_df.to_excel(writer, sheet_name=f\"predicion_accuracies\")\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl.drawing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "y = y_test\n",
    "log_y = log_y_actual\n",
    "\n",
    "raw_scores = np.array(raw_pred)\n",
    "combined_scores = np.array(combined_pred)\n",
    "\n",
    "raw_fpr, raw_tpr, raw_thresh = metrics.roc_curve(y, raw_scores, pos_label=1)\n",
    "raw_roc_auc = metrics.auc(raw_fpr, raw_tpr)\n",
    "\n",
    "com_fpr, com_tpr, com_thresh = metrics.roc_curve(y, combined_scores, pos_label=1)\n",
    "com_roc_auc = metrics.auc(com_fpr, com_tpr)\n",
    "\n",
    "raw_fpr_log, raw_tpr_log, raw_thresh_log = metrics.roc_curve(y, log_raw_pred, pos_label=1)\n",
    "raw_roc_auc_log = metrics.auc(raw_fpr_log, raw_tpr_log)\n",
    "\n",
    "com_fpr_log, com_tpr_log, com_thresh_log = metrics.roc_curve(y, log_combined_pred, pos_label=1)\n",
    "com_roc_auc_log = metrics.auc(com_fpr_log, com_tpr_log)\n",
    "\n",
    "raw_fpr_nb, raw_tpr_nb, raw_thresh_nb = metrics.roc_curve(y, nb_raw_pred, pos_label=1)\n",
    "raw_roc_auc_nb = metrics.auc(raw_fpr_nb, raw_tpr_nb)\n",
    "\n",
    "com_fpr_nb, com_tpr_nb, com_thresh_nb = metrics.roc_curve(y, nb_combined_pred, pos_label=1)\n",
    "com_roc_auc_nb = metrics.auc(com_fpr_nb, com_tpr_nb)\n",
    "\n",
    "raw_fpr_rf, raw_tpr_rf, raw_thresh_rf = metrics.roc_curve(y, rf_raw_pred, pos_label=1)\n",
    "raw_roc_auc_rf = metrics.auc(raw_fpr_rf, raw_tpr_rf)\n",
    "\n",
    "com_fpr_rf, com_tpr_rf, com_thresh_rf = metrics.roc_curve(y, rf_combined_pred, pos_label=1)\n",
    "com_roc_auc_rf = metrics.auc(com_fpr_rf, com_tpr_rf)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(raw_fpr, raw_tpr,\n",
    " lw=lw, label='Raw NN (%0.2f)' % raw_roc_auc)\n",
    "plt.plot(com_fpr, com_tpr,\n",
    " lw=lw, label='Sentiment Classifier NN  (%0.2f)' % com_roc_auc)\n",
    "plt.plot(raw_fpr_log, raw_tpr_log,\n",
    " lw=lw, label='Raw Log (%0.2f)' % raw_roc_auc_log)\n",
    "plt.plot(com_fpr_log, com_tpr_log,\n",
    " lw=lw, label='Sentiment Classifier Log  (%0.2f)' % com_roc_auc_log)\n",
    "\n",
    "plt.plot(raw_fpr_nb, raw_tpr_nb,\n",
    " lw=lw, label='Raw Naive Bayes (%0.2f)' % raw_roc_auc_nb)\n",
    "plt.plot(com_fpr_nb, com_tpr_nb,\n",
    " lw=lw, label='Sentiment Classifier Naive Bayes  (%0.2f)' % com_roc_auc_nb)\n",
    "\n",
    "plt.plot(raw_fpr_rf, raw_tpr_rf,\n",
    " lw=lw, label='Raw Naive Random Forest (%0.2f)' % raw_roc_auc_rf)\n",
    "plt.plot(com_fpr_rf, com_tpr_rf,\n",
    " lw=lw, label='Sentiment Classifier Random Forest  (%0.2f)' % com_roc_auc_rf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# save figure as PNG\n",
    "png = io.BytesIO()\n",
    "plt.savefig(png, format=\"png\")\n",
    "\n",
    "\n",
    "# write PNG to excel file\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "ws = book.active\n",
    "\n",
    "img = openpyxl.drawing.image.Image(png)\n",
    "img.anchor = \"A1\"\n",
    "ws.add_image(img)\n",
    "book.save(filename=EXCEL_FILE)\n",
    "plt.close()\n",
    "book.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
