{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    ISOT\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/ISOT_PROCESSED.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "'''\n",
    "    Define filtered dataset, classes, features, dataframe for model accuracies, and excel file for results\n",
    "'''\n",
    "\n",
    "'''Drop NA'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"flag\"].to_numpy()\n",
    "\n",
    "#just tweet sentiment classifier\n",
    "tweet_features = filtered[[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\"]].to_numpy()\n",
    "\n",
    "#just imbd sentiment classifier\n",
    "imdb_features = filtered[[\"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]].to_numpy()\n",
    "\n",
    "# both sentiment classifiers\n",
    "tweet_and_imdb = filtered[[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\",\n",
    "                           \"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "raw_features = filtered[[\"text_tb_pol\",\t\"text_tb_sub\",\t\"title_tb_pol\",\t\"title_tb_sub\",\t\"title_vader_comp\",\t\"title_vader_neg\",\t\n",
    "              \"title_vader_neu\",\t\"title_vader_pos\",\t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "# dataframe to store accuracies for NN and log regression\n",
    "accuracy_df = pd.DataFrame(columns=[\"tweet_classifier\",\n",
    "               \"imdb_classifier\",\n",
    "               \"combined_classifier\",\n",
    "               \"raw_sentiments\"])\n",
    "\n",
    "\n",
    "EXCEL_FILE = r\"../data/Des_fake_news/Sentiment_Analysis_Results/ISOT_RESULTS.xlsx\"\n",
    "# overwrite book if exists\n",
    "book = Workbook()\n",
    "book.save(filename=EXCEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_12844\\925897228.py:72: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_classifier</th>\n",
       "      <th>imdb_classifier</th>\n",
       "      <th>combined_classifier</th>\n",
       "      <th>raw_sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.640172</td>\n",
       "      <td>0.525776</td>\n",
       "      <td>0.640024</td>\n",
       "      <td>0.709404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                tweet_classifier  imdb_classifier  combined_classifier  \\\n",
       "Log Regression          0.640172         0.525776             0.640024   \n",
       "\n",
       "                raw_sentiments  \n",
       "Log Regression        0.709404  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "  Logistic Regression work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "logOutput = {}\n",
    "logMatrices = {\"tweet_classifier\" : [],\n",
    "               \"imdb_classifier\" : [],\n",
    "               \"combined_classifier\": [],\n",
    "               \"raw_sentiments\" : []}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "# run 100 iterations\n",
    "for i in range(1):\n",
    "  print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "  x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=i)\n",
    "  lbgfs.fit(x_train, y_train)\n",
    "  y_pred = lbgfs.predict(x_test)\n",
    "  tweet_scores += accuracy_score(y_test, y_pred)\n",
    "  logMatrices[\"tweet_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=i)\n",
    "  lbgfs.fit(x_train, y_train)\n",
    "  y_pred = lbgfs.predict(x_test)\n",
    "  imdb_scores += accuracy_score(y_test, y_pred)\n",
    "  logMatrices[\"imdb_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=i)\n",
    "  lbgfs.fit(x_train, y_train)\n",
    "  y_pred = lbgfs.predict(x_test)\n",
    "  tweet_imdb_scores += accuracy_score(y_test, y_pred)\n",
    "  logMatrices[\"combined_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "  x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=i)\n",
    "  lbgfs.fit(x_train, y_train)\n",
    "  y_pred = lbgfs.predict(x_test)\n",
    "  raw_scores += accuracy_score(y_test, lbgfs.predict(x_test))\n",
    "  logMatrices[\"raw_sentiments\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "logOutput[\"tweet_classifier\"] = tweet_scores / 1\n",
    "logOutput[\"imdb_classifier\"] = imdb_scores / 1\n",
    "logOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "logOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "logMatrices[\"tweet_classifier\"] = np.mean(np.array(logMatrices[\"tweet_classifier\"]), axis=0)\n",
    "logMatrices[\"imdb_classifier\"] = np.mean(np.array(logMatrices[\"imdb_classifier\"]), axis=0)\n",
    "logMatrices[\"combined_classifier\"] = np.mean(np.array(logMatrices[\"combined_classifier\"]), axis=0)\n",
    "logMatrices[\"raw_sentiments\"] = np.mean(np.array(logMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in logMatrices.keys():\n",
    "  pd.DataFrame(logMatrices[i]).to_excel(writer, sheet_name=f\"matrix_log_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "\n",
    "accuracy_df.loc[\"Log Regression\"] = logOutput\n",
    "accuracy_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and save neural net models\n",
    "'''\n",
    "DIR = r\"Fake_news_nn\"\n",
    "import tensorflow as tf\n",
    "saved_models = os.listdir(DIR)\n",
    "\n",
    "tweets_name = \"ISOT_tweets.keras\"\n",
    "imdb_name = \"ISOT_imdb.keras\"\n",
    "combined_name = \"ISOT_combined.keras\"\n",
    "raw_name = \"ISOT_raw.keras\"\n",
    "\n",
    "tweet_len = tweet_features.shape[1]\n",
    "imdb_len = imdb_features.shape[1]\n",
    "combined_len = tweet_and_imdb.shape[1]\n",
    "raw_len = raw_features.shape[1]\n",
    "\n",
    "\n",
    "#if tweets_name not in saved_models:\n",
    "tweet_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(tweet_len, 1)),\n",
    "  tf.keras.layers.Dense(tweet_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "tweet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  #tweet_model.save(f\"{DIR}/{tweets_name}\")\n",
    "\n",
    "#if imdb_name not in saved_models:\n",
    "imdb_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(imdb_len, 1)),\n",
    "  tf.keras.layers.Dense(imdb_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "imdb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#imdb_model.save(f\"{DIR}/{imdb_name}\")\n",
    "\n",
    "#if combined_name not in saved_models:\n",
    "combined_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(combined_len, 1)),\n",
    "  tf.keras.layers.Dense(combined_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#combined_model.save(f\"{DIR}/{combined_name}\")\n",
    "\n",
    "#if raw_name not in saved_models:\n",
    "raw_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(raw_len, 1)),\n",
    "  tf.keras.layers.Dense(raw_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "raw_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#raw_model.save(f\"{DIR}/{raw_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7692 - loss: 0.4871\n",
      "Epoch 2/100\n",
      "1192/1192 - 1s - 996us/step - accuracy: 0.7730 - loss: 0.4828\n",
      "Epoch 3/100\n",
      "1192/1192 - 1s - 969us/step - accuracy: 0.7736 - loss: 0.4794\n",
      "Epoch 4/100\n",
      "1192/1192 - 1s - 974us/step - accuracy: 0.7761 - loss: 0.4767\n",
      "Epoch 5/100\n",
      "1192/1192 - 1s - 954us/step - accuracy: 0.7775 - loss: 0.4742\n",
      "Epoch 6/100\n",
      "1192/1192 - 1s - 950us/step - accuracy: 0.7781 - loss: 0.4723\n",
      "Epoch 7/100\n",
      "1192/1192 - 1s - 955us/step - accuracy: 0.7796 - loss: 0.4706\n",
      "Epoch 8/100\n",
      "1192/1192 - 1s - 961us/step - accuracy: 0.7799 - loss: 0.4692\n",
      "Epoch 9/100\n",
      "1192/1192 - 1s - 988us/step - accuracy: 0.7807 - loss: 0.4682\n",
      "Epoch 10/100\n",
      "1192/1192 - 1s - 962us/step - accuracy: 0.7821 - loss: 0.4671\n",
      "Epoch 11/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7823 - loss: 0.4660\n",
      "Epoch 12/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7823 - loss: 0.4651\n",
      "Epoch 13/100\n",
      "1192/1192 - 1s - 973us/step - accuracy: 0.7822 - loss: 0.4645\n",
      "Epoch 14/100\n",
      "1192/1192 - 1s - 973us/step - accuracy: 0.7821 - loss: 0.4641\n",
      "Epoch 15/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7821 - loss: 0.4636\n",
      "Epoch 16/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7830 - loss: 0.4628\n",
      "Epoch 17/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7832 - loss: 0.4622\n",
      "Epoch 18/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7843 - loss: 0.4618\n",
      "Epoch 19/100\n",
      "1192/1192 - 1s - 973us/step - accuracy: 0.7834 - loss: 0.4610\n",
      "Epoch 20/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7839 - loss: 0.4606\n",
      "Epoch 21/100\n",
      "1192/1192 - 1s - 976us/step - accuracy: 0.7825 - loss: 0.4602\n",
      "Epoch 22/100\n",
      "1192/1192 - 1s - 971us/step - accuracy: 0.7832 - loss: 0.4596\n",
      "Epoch 23/100\n",
      "1192/1192 - 1s - 977us/step - accuracy: 0.7843 - loss: 0.4588\n",
      "Epoch 24/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7841 - loss: 0.4586\n",
      "Epoch 25/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7851 - loss: 0.4580\n",
      "Epoch 26/100\n",
      "1192/1192 - 1s - 996us/step - accuracy: 0.7850 - loss: 0.4579\n",
      "Epoch 27/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7854 - loss: 0.4571\n",
      "Epoch 28/100\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.7845 - loss: 0.4567\n",
      "Epoch 29/100\n",
      "1192/1192 - 1s - 983us/step - accuracy: 0.7855 - loss: 0.4567\n",
      "Epoch 30/100\n",
      "1192/1192 - 1s - 986us/step - accuracy: 0.7855 - loss: 0.4561\n",
      "Epoch 31/100\n",
      "1192/1192 - 1s - 989us/step - accuracy: 0.7853 - loss: 0.4558\n",
      "Epoch 32/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7848 - loss: 0.4554\n",
      "Epoch 33/100\n",
      "1192/1192 - 1s - 953us/step - accuracy: 0.7852 - loss: 0.4553\n",
      "Epoch 34/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7856 - loss: 0.4550\n",
      "Epoch 35/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7849 - loss: 0.4549\n",
      "Epoch 36/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7847 - loss: 0.4547\n",
      "Epoch 37/100\n",
      "1192/1192 - 1s - 975us/step - accuracy: 0.7866 - loss: 0.4545\n",
      "Epoch 38/100\n",
      "1192/1192 - 1s - 981us/step - accuracy: 0.7848 - loss: 0.4544\n",
      "Epoch 39/100\n",
      "1192/1192 - 1s - 966us/step - accuracy: 0.7863 - loss: 0.4541\n",
      "Epoch 40/100\n",
      "1192/1192 - 1s - 964us/step - accuracy: 0.7856 - loss: 0.4536\n",
      "Epoch 41/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7860 - loss: 0.4535\n",
      "Epoch 42/100\n",
      "1192/1192 - 1s - 982us/step - accuracy: 0.7868 - loss: 0.4537\n",
      "Epoch 43/100\n",
      "1192/1192 - 1s - 986us/step - accuracy: 0.7866 - loss: 0.4528\n",
      "Epoch 44/100\n",
      "1192/1192 - 1s - 986us/step - accuracy: 0.7860 - loss: 0.4529\n",
      "Epoch 45/100\n",
      "1192/1192 - 1s - 966us/step - accuracy: 0.7857 - loss: 0.4524\n",
      "Epoch 46/100\n",
      "1192/1192 - 1s - 962us/step - accuracy: 0.7870 - loss: 0.4529\n",
      "Epoch 47/100\n",
      "1192/1192 - 1s - 965us/step - accuracy: 0.7856 - loss: 0.4526\n",
      "Epoch 48/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7863 - loss: 0.4522\n",
      "Epoch 49/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7856 - loss: 0.4519\n",
      "Epoch 50/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7880 - loss: 0.4516\n",
      "Epoch 51/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7870 - loss: 0.4515\n",
      "Epoch 52/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7859 - loss: 0.4514\n",
      "Epoch 53/100\n",
      "1192/1192 - 1s - 973us/step - accuracy: 0.7863 - loss: 0.4517\n",
      "Epoch 54/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7861 - loss: 0.4513\n",
      "Epoch 55/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7880 - loss: 0.4510\n",
      "Epoch 56/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7876 - loss: 0.4505\n",
      "Epoch 57/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7874 - loss: 0.4510\n",
      "Epoch 58/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7874 - loss: 0.4507\n",
      "Epoch 59/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7873 - loss: 0.4505\n",
      "Epoch 60/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7881 - loss: 0.4503\n",
      "Epoch 61/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7870 - loss: 0.4504\n",
      "Epoch 62/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7871 - loss: 0.4502\n",
      "Epoch 63/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7878 - loss: 0.4500\n",
      "Epoch 64/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7881 - loss: 0.4494\n",
      "Epoch 65/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7880 - loss: 0.4497\n",
      "Epoch 66/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7877 - loss: 0.4494\n",
      "Epoch 67/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7875 - loss: 0.4496\n",
      "Epoch 68/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7869 - loss: 0.4492\n",
      "Epoch 69/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7883 - loss: 0.4491\n",
      "Epoch 70/100\n",
      "1192/1192 - 1s - 999us/step - accuracy: 0.7873 - loss: 0.4491\n",
      "Epoch 71/100\n",
      "1192/1192 - 1s - 996us/step - accuracy: 0.7872 - loss: 0.4491\n",
      "Epoch 72/100\n",
      "1192/1192 - 1s - 978us/step - accuracy: 0.7884 - loss: 0.4490\n",
      "Epoch 73/100\n",
      "1192/1192 - 1s - 964us/step - accuracy: 0.7871 - loss: 0.4490\n",
      "Epoch 74/100\n",
      "1192/1192 - 1s - 965us/step - accuracy: 0.7878 - loss: 0.4485\n",
      "Epoch 75/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7884 - loss: 0.4482\n",
      "Epoch 76/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7873 - loss: 0.4484\n",
      "Epoch 77/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7882 - loss: 0.4483\n",
      "Epoch 78/100\n",
      "1192/1192 - 1s - 984us/step - accuracy: 0.7878 - loss: 0.4482\n",
      "Epoch 79/100\n",
      "1192/1192 - 1s - 979us/step - accuracy: 0.7885 - loss: 0.4482\n",
      "Epoch 80/100\n",
      "1192/1192 - 1s - 981us/step - accuracy: 0.7884 - loss: 0.4481\n",
      "Epoch 81/100\n",
      "1192/1192 - 1s - 978us/step - accuracy: 0.7881 - loss: 0.4481\n",
      "Epoch 82/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7884 - loss: 0.4476\n",
      "Epoch 83/100\n",
      "1192/1192 - 1s - 953us/step - accuracy: 0.7876 - loss: 0.4478\n",
      "Epoch 84/100\n",
      "1192/1192 - 1s - 949us/step - accuracy: 0.7886 - loss: 0.4479\n",
      "Epoch 85/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7878 - loss: 0.4481\n",
      "Epoch 86/100\n",
      "1192/1192 - 1s - 981us/step - accuracy: 0.7885 - loss: 0.4475\n",
      "Epoch 87/100\n",
      "1192/1192 - 1s - 992us/step - accuracy: 0.7873 - loss: 0.4474\n",
      "Epoch 88/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7875 - loss: 0.4476\n",
      "Epoch 89/100\n",
      "1192/1192 - 1s - 1000us/step - accuracy: 0.7870 - loss: 0.4474\n",
      "Epoch 90/100\n",
      "1192/1192 - 1s - 968us/step - accuracy: 0.7880 - loss: 0.4476\n",
      "Epoch 91/100\n",
      "1192/1192 - 1s - 962us/step - accuracy: 0.7897 - loss: 0.4471\n",
      "Epoch 92/100\n",
      "1192/1192 - 1s - 977us/step - accuracy: 0.7878 - loss: 0.4475\n",
      "Epoch 93/100\n",
      "1192/1192 - 1s - 974us/step - accuracy: 0.7883 - loss: 0.4476\n",
      "Epoch 94/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7884 - loss: 0.4469\n",
      "Epoch 95/100\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7881 - loss: 0.4469\n",
      "Epoch 96/100\n",
      "1192/1192 - 1s - 978us/step - accuracy: 0.7872 - loss: 0.4469\n",
      "Epoch 97/100\n",
      "1192/1192 - 1s - 980us/step - accuracy: 0.7871 - loss: 0.4467\n",
      "Epoch 98/100\n",
      "1192/1192 - 1s - 972us/step - accuracy: 0.7881 - loss: 0.4467\n",
      "Epoch 99/100\n",
      "1192/1192 - 1s - 979us/step - accuracy: 0.7889 - loss: 0.4468\n",
      "Epoch 100/100\n",
      "1192/1192 - 1s - 962us/step - accuracy: 0.7885 - loss: 0.4464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x24f22e0cc10>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    NN results\n",
    "'''\n",
    "\n",
    "# tweet NN\n",
    "#x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "#tweet_model.fit(x_train, y_train, epochs=10, verbose=2)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "raw_model.fit(x_train, y_train, epochs=100, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7880 - loss: 0.4479\n",
      "211/211 - 0s - 955us/step - accuracy: 0.7884 - loss: 0.4515\n",
      "Train / Test Accuracy: 78.8% / 78.8%\n"
     ]
    }
   ],
   "source": [
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "print(f\"Train / Test Accuracy: {model_acc1*100:.1f}% / {model_acc2*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5214678353885009"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = raw_model.predict(x_test)\n",
    "predicted = [1 if i >= 0.50 else 0 for i in predicted]\n",
    "matrix = confusion_matrix(y_test, predicted)\n",
    "\n",
    "uniques, counts = np.unique(y_test, return_counts=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
