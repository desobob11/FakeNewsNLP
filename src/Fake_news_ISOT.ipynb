{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "'''\n",
    "    ISOT\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "data = pd.read_csv(r\"../data/Des_fake_news/ISOT_PROCESSED.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "'''\n",
    "    Define filtered dataset, classes, features, dataframe for model accuracies, and excel file for results\n",
    "'''\n",
    "\n",
    "'''Drop NA'''\n",
    "filtered = data.dropna()\n",
    "classes = filtered[\"flag\"].to_numpy()\n",
    "\n",
    "#just tweet sentiment classifier\n",
    "tweet_features = filtered[[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\"]].to_numpy()\n",
    "\n",
    "#just imbd sentiment classifier\n",
    "imdb_features = filtered[[\"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]].to_numpy()\n",
    "\n",
    "# both sentiment classifiers\n",
    "tweet_and_imdb = filtered[[\"text_NN_tweets\", \"title_NN_tweets\", \"text_log_tweets\", \"title_log_tweets\", \"text_tb_sub_class\", \"title_tb_sub_class\",\n",
    "                           \"text_NN_imdb\", \"title_NN_imdb\", \"text_log_imdb\", \"title_log_imdb\"]].to_numpy()\n",
    "\n",
    "# raw polarity and subjectivity scores from Textblob, Vader\n",
    "raw_features = filtered[[\"text_tb_pol\",\t\"text_tb_sub\",\t\"title_tb_pol\",\t\"title_tb_sub\",\t\"title_vader_comp\",\t\"title_vader_neg\",\t\n",
    "              \"title_vader_neu\",\t\"title_vader_pos\",\t\"text_vader_comp\",\t\"text_vader_neg\",\t\"text_vader_neu\",\t\"text_vader_pos\"]].to_numpy()\n",
    "\n",
    "# dataframe to store accuracies for NN and log regression\n",
    "accuracy_df = pd.DataFrame(columns=[\"tweet_classifier\",\n",
    "               \"imdb_classifier\",\n",
    "               \"combined_classifier\",\n",
    "               \"raw_sentiments\"])\n",
    "\n",
    "\n",
    "EXCEL_FILE = r\"../data/Des_fake_news/Sentiment_Analysis_Results/ISOT_RESULTS.xlsx\"\n",
    "# overwrite book if exists\n",
    "book = Workbook()\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_5320\\346013611.py:78: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "  Logistic Regression work\n",
    "'''\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# average accuracies\n",
    "logOutput = {}\n",
    "logMatrices = {\"tweet_classifier\" : [],\n",
    "               \"imdb_classifier\" : [],\n",
    "               \"combined_classifier\": [],\n",
    "               \"raw_sentiments\" : []}\n",
    "\n",
    "# sums\n",
    "tweet_scores = 0\n",
    "imdb_scores = 0\n",
    "tweet_imdb_scores = 0\n",
    "raw_scores = 0\n",
    "\n",
    "# log regression model, LBFGS with L2 penalty\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "\n",
    "log_combined_pred = None\n",
    "log_raw_pred = None\n",
    "\n",
    "# run 100 iterations\n",
    "#for i in range(1):\n",
    "  #print(f\"Iteration {i}\")\n",
    "  #tweet_accuracy\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "tweet_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"tweet_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "y_pred = lbgfs.predict(x_test)\n",
    "imdb_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"imdb_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_combined_pred = lbgfs.predict(x_test)\n",
    "tweet_imdb_scores += accuracy_score(y_test, y_pred)\n",
    "logMatrices[\"combined_classifier\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "lbgfs.fit(x_train, y_train)\n",
    "log_raw_pred = lbgfs.predict(x_test)\n",
    "raw_scores += accuracy_score(y_test, lbgfs.predict(x_test))\n",
    "logMatrices[\"raw_sentiments\"].append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "log_y_actual = y_test\n",
    "\n",
    "logOutput[\"tweet_classifier\"] = tweet_scores / 1\n",
    "logOutput[\"imdb_classifier\"] = imdb_scores / 1\n",
    "logOutput[\"combined_classifier\"] = tweet_imdb_scores / 1\n",
    "logOutput[\"raw_sentiments\"] = raw_scores / 1\n",
    "\n",
    "logMatrices[\"tweet_classifier\"] = np.mean(np.array(logMatrices[\"tweet_classifier\"]), axis=0)\n",
    "logMatrices[\"imdb_classifier\"] = np.mean(np.array(logMatrices[\"imdb_classifier\"]), axis=0)\n",
    "logMatrices[\"combined_classifier\"] = np.mean(np.array(logMatrices[\"combined_classifier\"]), axis=0)\n",
    "logMatrices[\"raw_sentiments\"] = np.mean(np.array(logMatrices[\"raw_sentiments\"]), axis=0)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in logMatrices.keys():\n",
    "  pd.DataFrame(logMatrices[i]).to_excel(writer, sheet_name=f\"matrix_log_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n",
    "accuracy_df.loc[\"Log Regression\"] = logOutput\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\desmo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Compile and save neural net models\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tweet_len = tweet_features.shape[1]\n",
    "imdb_len = imdb_features.shape[1]\n",
    "combined_len = tweet_and_imdb.shape[1]\n",
    "raw_len = raw_features.shape[1]\n",
    "\n",
    "\n",
    "tweet_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(tweet_len, 1)),\n",
    "  tf.keras.layers.Dense(tweet_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "tweet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "imdb_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(imdb_len, 1)),\n",
    "  tf.keras.layers.Dense(imdb_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "imdb_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "combined_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(combined_len, 1)),\n",
    "  tf.keras.layers.Dense(combined_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "combined_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "raw_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(raw_len, 1)),\n",
    "  tf.keras.layers.Dense(raw_len, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "raw_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1192/1192 - 3s - 3ms/step - accuracy: 0.6199 - loss: 0.6780\n",
      "1192/1192 - 1s - 992us/step - accuracy: 0.6533 - loss: 0.6220\n",
      "211/211 - 0s - 959us/step - accuracy: 0.6485 - loss: 0.6239\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.5125 - loss: 0.6921\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.5122 - loss: 0.6914\n",
      "211/211 - 0s - 972us/step - accuracy: 0.5148 - loss: 0.6917\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6411 - loss: 0.6311\n",
      "Epoch 2/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6584 - loss: 0.6138\n",
      "Epoch 3/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.6593 - loss: 0.6123\n",
      "Epoch 4/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6608 - loss: 0.6115\n",
      "Epoch 5/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6614 - loss: 0.6107\n",
      "Epoch 6/40\n",
      "1192/1192 - 3s - 3ms/step - accuracy: 0.6624 - loss: 0.6102\n",
      "Epoch 7/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6624 - loss: 0.6100\n",
      "Epoch 8/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6620 - loss: 0.6101\n",
      "Epoch 9/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6634 - loss: 0.6098\n",
      "Epoch 10/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6632 - loss: 0.6096\n",
      "Epoch 11/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6639 - loss: 0.6096\n",
      "Epoch 12/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6633 - loss: 0.6096\n",
      "Epoch 13/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6637 - loss: 0.6094\n",
      "Epoch 14/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6632 - loss: 0.6094\n",
      "Epoch 15/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6633 - loss: 0.6093\n",
      "Epoch 16/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6619 - loss: 0.6093\n",
      "Epoch 17/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6634 - loss: 0.6090\n",
      "Epoch 18/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.6632 - loss: 0.6091\n",
      "Epoch 19/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6628 - loss: 0.6090\n",
      "Epoch 20/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6635 - loss: 0.6090\n",
      "Epoch 21/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6632 - loss: 0.6090\n",
      "Epoch 22/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6629 - loss: 0.6092\n",
      "Epoch 23/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6632 - loss: 0.6088\n",
      "Epoch 24/40\n",
      "1192/1192 - 3s - 2ms/step - accuracy: 0.6634 - loss: 0.6089\n",
      "Epoch 25/40\n",
      "1192/1192 - 3s - 2ms/step - accuracy: 0.6631 - loss: 0.6090\n",
      "Epoch 26/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6636 - loss: 0.6089\n",
      "Epoch 27/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6632 - loss: 0.6089\n",
      "Epoch 28/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6628 - loss: 0.6088\n",
      "Epoch 29/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6642 - loss: 0.6089\n",
      "Epoch 30/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6636 - loss: 0.6089\n",
      "Epoch 31/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6634 - loss: 0.6090\n",
      "Epoch 32/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6635 - loss: 0.6089\n",
      "Epoch 33/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6642 - loss: 0.6088\n",
      "Epoch 34/40\n",
      "1192/1192 - 1s - 986us/step - accuracy: 0.6639 - loss: 0.6087\n",
      "Epoch 35/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6640 - loss: 0.6086\n",
      "Epoch 36/40\n",
      "1192/1192 - 1s - 994us/step - accuracy: 0.6640 - loss: 0.6088\n",
      "Epoch 37/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6635 - loss: 0.6087\n",
      "Epoch 38/40\n",
      "1192/1192 - 1s - 987us/step - accuracy: 0.6631 - loss: 0.6088\n",
      "Epoch 39/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.6634 - loss: 0.6087\n",
      "Epoch 40/40\n",
      "1192/1192 - 1s - 993us/step - accuracy: 0.6639 - loss: 0.6086\n",
      "1192/1192 - 1s - 973us/step - accuracy: 0.6641 - loss: 0.6083\n",
      "211/211 - 0s - 972us/step - accuracy: 0.6538 - loss: 0.6129\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Epoch 1/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.6617 - loss: 0.6168\n",
      "Epoch 2/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7063 - loss: 0.5755\n",
      "Epoch 3/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7211 - loss: 0.5550\n",
      "Epoch 4/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7310 - loss: 0.5406\n",
      "Epoch 5/40\n",
      "1192/1192 - 3s - 2ms/step - accuracy: 0.7386 - loss: 0.5293\n",
      "Epoch 6/40\n",
      "1192/1192 - 5s - 4ms/step - accuracy: 0.7431 - loss: 0.5212\n",
      "Epoch 7/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.7468 - loss: 0.5146\n",
      "Epoch 8/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7520 - loss: 0.5084\n",
      "Epoch 9/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7540 - loss: 0.5039\n",
      "Epoch 10/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7591 - loss: 0.4990\n",
      "Epoch 11/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7614 - loss: 0.4946\n",
      "Epoch 12/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7651 - loss: 0.4903\n",
      "Epoch 13/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7693 - loss: 0.4839\n",
      "Epoch 14/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7728 - loss: 0.4787\n",
      "Epoch 15/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7744 - loss: 0.4754\n",
      "Epoch 16/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7780 - loss: 0.4725\n",
      "Epoch 17/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7779 - loss: 0.4702\n",
      "Epoch 18/40\n",
      "1192/1192 - 2s - 1ms/step - accuracy: 0.7784 - loss: 0.4683\n",
      "Epoch 19/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7785 - loss: 0.4667\n",
      "Epoch 20/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7787 - loss: 0.4649\n",
      "Epoch 21/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7799 - loss: 0.4636\n",
      "Epoch 22/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7796 - loss: 0.4624\n",
      "Epoch 23/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7810 - loss: 0.4606\n",
      "Epoch 24/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7824 - loss: 0.4596\n",
      "Epoch 25/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7819 - loss: 0.4586\n",
      "Epoch 26/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7816 - loss: 0.4579\n",
      "Epoch 27/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7817 - loss: 0.4569\n",
      "Epoch 28/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7834 - loss: 0.4561\n",
      "Epoch 29/40\n",
      "1192/1192 - 3s - 2ms/step - accuracy: 0.7842 - loss: 0.4555\n",
      "Epoch 30/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7841 - loss: 0.4550\n",
      "Epoch 31/40\n",
      "1192/1192 - 3s - 2ms/step - accuracy: 0.7844 - loss: 0.4544\n",
      "Epoch 32/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.7854 - loss: 0.4537\n",
      "Epoch 33/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7856 - loss: 0.4536\n",
      "Epoch 34/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.7851 - loss: 0.4530\n",
      "Epoch 35/40\n",
      "1192/1192 - 2s - 2ms/step - accuracy: 0.7859 - loss: 0.4531\n",
      "Epoch 36/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7862 - loss: 0.4523\n",
      "Epoch 37/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7852 - loss: 0.4519\n",
      "Epoch 38/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7860 - loss: 0.4519\n",
      "Epoch 39/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7868 - loss: 0.4511\n",
      "Epoch 40/40\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7876 - loss: 0.4506\n",
      "1192/1192 - 1s - 1ms/step - accuracy: 0.7881 - loss: 0.4485\n",
      "211/211 - 0s - 1ms/step - accuracy: 0.7892 - loss: 0.4468\n",
      "\u001b[1m211/211\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_5320\\491802460.py:53: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    NN results\n",
    "'''\n",
    "\n",
    "EPOCHS = 40\n",
    "nnOutput = {}\n",
    "nnMatrices = {\"tweet_classifier\" : None,\n",
    "               \"imdb_classifier\" : None,\n",
    "               \"combined_classifier\": None,\n",
    "               \"raw_sentiments\" : None}\n",
    "\n",
    "\n",
    "# tweet NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_features, classes, test_size=0.15, random_state=42)\n",
    "tweet_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = tweet_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = tweet_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"tweet_classifier\"] = model_acc2\n",
    "tweet_pred = [1 if i >= 0.5 else 0 for i in tweet_model.predict(x_test)]\n",
    "nnMatrices[\"tweet_classifier\"] = confusion_matrix(y_test, tweet_pred)\n",
    "\n",
    "# imdb NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(imdb_features, classes, test_size=0.15, random_state=42)\n",
    "imdb_model.fit(x_train, y_train, epochs=1, verbose=2)\n",
    "model_loss1, model_acc1 = imdb_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = imdb_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"imdb_classifier\"] = model_acc2\n",
    "imdb_pred = [1 if i >= 0.5 else 0 for i in imdb_model.predict(x_test)]\n",
    "nnMatrices[\"imdb_classifier\"] = confusion_matrix(y_test, imdb_pred)\n",
    "\n",
    "# combined NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(tweet_and_imdb, classes, test_size=0.15, random_state=42)\n",
    "combined_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = combined_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = combined_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"combined_classifier\"] = model_acc2\n",
    "combined_pred = [1 if i >= 0.5 else 0 for i in combined_model.predict(x_test)]\n",
    "nnMatrices[\"combined_classifier\"] = confusion_matrix(y_test, combined_pred)\n",
    "\n",
    "# raw NN\n",
    "x_train, x_test, y_train, y_test = train_test_split(raw_features, classes, test_size=0.15, random_state=42)\n",
    "raw_model.fit(x_train, y_train, epochs=EPOCHS, verbose=2)\n",
    "model_loss1, model_acc1 = raw_model.evaluate(x_train,  y_train, verbose=2)\n",
    "model_loss2, model_acc2 = raw_model.evaluate(x_test,  y_test, verbose=2)\n",
    "nnOutput[\"raw_sentiments\"] = model_acc2\n",
    "raw_pred = [1 if i >= 0.5 else 0 for i in raw_model.predict(x_test)]\n",
    "nnMatrices[\"raw_sentiments\"] = confusion_matrix(y_test, raw_pred)\n",
    "\n",
    "\n",
    "# write confusion matrices and save\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "\n",
    "for i in nnMatrices.keys():\n",
    "  pd.DataFrame(nnMatrices[i]).to_excel(writer, sheet_name=f\"matrix_NN_{i}\")\n",
    "\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "accuracy_df.loc[\"Neural Net\"] = nnOutput\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\desmo\\AppData\\Local\\Temp\\ipykernel_5320\\3554747512.py:7: FutureWarning: Setting the `book` attribute is not part of the public API, usage can give unexpected or corrupted results and will be removed in a future version\n",
      "  writer.book = book\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Finally, save accuracy metrics to the spreadsheet\n",
    "'''\n",
    "\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "writer = pd.ExcelWriter(EXCEL_FILE, engine=\"openpyxl\")\n",
    "writer.book = book\n",
    "accuracy_df.to_excel(writer, sheet_name=f\"predicion_accuracies\")\n",
    "book.save(filename=EXCEL_FILE)\n",
    "book.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl.drawing\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "y = y_test\n",
    "log_y = log_y_actual\n",
    "\n",
    "raw_scores = np.array(raw_pred)\n",
    "combined_scores = np.array(combined_pred)\n",
    "\n",
    "raw_fpr, raw_tpr, raw_thresh = metrics.roc_curve(y, raw_scores, pos_label=1)\n",
    "raw_roc_auc = metrics.auc(raw_fpr, raw_tpr)\n",
    "\n",
    "com_fpr, com_tpr, com_thresh = metrics.roc_curve(y, combined_scores, pos_label=1)\n",
    "com_roc_auc = metrics.auc(com_fpr, com_tpr)\n",
    "\n",
    "raw_fpr_log, raw_tpr_log, raw_thresh_log = metrics.roc_curve(y, log_raw_pred, pos_label=1)\n",
    "raw_roc_auc_log = metrics.auc(raw_fpr_log, raw_tpr_log)\n",
    "\n",
    "com_fpr_log, com_tpr_log, com_thresh_log = metrics.roc_curve(y, log_combined_pred, pos_label=1)\n",
    "com_roc_auc_log = metrics.auc(com_fpr_log, com_tpr_log)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(raw_fpr, raw_tpr,\n",
    " lw=lw, label='Raw NN (%0.2f)' % raw_roc_auc)\n",
    "plt.plot(com_fpr, com_tpr,\n",
    " lw=lw, label='Sentiment Classifier NN  (%0.2f)' % com_roc_auc)\n",
    "plt.plot(raw_fpr_log, raw_tpr_log,\n",
    " lw=lw, label='Raw Log (%0.2f)' % raw_roc_auc_log)\n",
    "plt.plot(com_fpr_log, com_tpr_log,\n",
    " lw=lw, label='Sentiment Classifier Log  (%0.2f)' % com_roc_auc_log)\n",
    "plt.plot([0, 1], [0, 1], color='black', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "# save figure as PNG\n",
    "png = io.BytesIO()\n",
    "plt.savefig(png, format=\"png\")\n",
    "\n",
    "\n",
    "# write PNG to excel file\n",
    "book = load_workbook(EXCEL_FILE)\n",
    "ws = book.active\n",
    "\n",
    "img = openpyxl.drawing.image.Image(png)\n",
    "img.anchor = \"A1\"\n",
    "ws.add_image(img)\n",
    "book.save(filename=EXCEL_FILE)\n",
    "plt.close()\n",
    "book.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
