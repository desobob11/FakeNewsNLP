{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from textblob import *\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import openpyxl\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import *\n",
    "\n",
    "\n",
    "training = pd.read_excel(r\"../../data/Des_SA/sentimentdataset.xlsx\", \"sentimentdataset\")\n",
    "\n",
    "validation = pd.read_excel(r\"../../data/Des_SA/sentimentdataset.xlsx\", \"validation\")\n",
    "testing = pd.read_excel(r\"../../data/Des_SA/sentimentdataset.xlsx\", \"testing\")\n",
    "\n",
    "#remove nonASCIICharacters\n",
    "training[\"Text\"] = training.apply(lambda x: \"\".join([i for i in x[\"Text\"] if ord(i) <= 127]), axis=1)\n",
    "validation[\"Text\"] = validation.apply(lambda x: \"\".join([i for i in x[\"Text\"] if ord(i) <= 127]), axis=1)\n",
    "testing[\"Text\"] = testing.apply(lambda x: \"\".join([i for i in x[\"Text\"] if ord(i) <= 127]), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_class(s):\n",
    "    match s:\n",
    "        case \"Positive\":\n",
    "            return 2\n",
    "        case \"Negative\":\n",
    "            return 1\n",
    "        case \"Neutral\":\n",
    "            return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"SimpleSent\"] = training.apply(lambda x: to_class(x[\"SimpleSent\"]), axis=1)\n",
    "testing[\"SimpleSent\"] = testing.apply(lambda x: to_class(x[\"SimpleSent\"]), axis=1)\n",
    "validation[\"SimpleSent\"] = validation.apply(lambda x: to_class(x[\"SimpleSent\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDrop = [\"Timestamp\", \"User\", \"Platform\", \"Retweets\", \"Likes\", \"Country\", \"Year\", \"Month\", \"Day\", \"Hour\"]\n",
    "\n",
    "training = training.drop(toDrop, axis=1)\n",
    "testing = testing.drop(toDrop, axis=1)\n",
    "validation = validation.drop(toDrop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_words(xs):\n",
    "    empties = []\n",
    "    empty = \"\" \n",
    "    for i in xs:\n",
    "        words = re.split(r\"([A-Z])\", i)\n",
    "        words = words[1::]\n",
    "        for i in range(0, len(words) -1, 2):\n",
    "            empty += words[i] + words[i + 1]\n",
    "            empty += \" \"\n",
    "        empties.append(empty.rstrip())\n",
    "    return empties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SimpleSent</th>\n",
       "      <th>Hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After a series of defeats, the soccer team fac...</td>\n",
       "      <td>Disappointment</td>\n",
       "      <td>1</td>\n",
       "      <td>Disappointment and Disappointment Soccer Defeats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the tennis tournament, a highly anticipated...</td>\n",
       "      <td>Frustration</td>\n",
       "      <td>1</td>\n",
       "      <td>Frustration and Frustration Tennis Setback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facing a defeat in the championship, the boxer...</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>0</td>\n",
       "      <td>Reflection and Reflection Boxing Defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the midst of a cycling race, a tire blowout...</td>\n",
       "      <td>Obstacle</td>\n",
       "      <td>0</td>\n",
       "      <td>Obstacle and Obstacle Cycling Frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The gymnast's unexpected fall during a routine...</td>\n",
       "      <td>Sympathy</td>\n",
       "      <td>2</td>\n",
       "      <td>Sympathy and Sympathy Gymnastics Fall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Organized a community painting event, turning ...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>2</td>\n",
       "      <td>Community Art and Community Art Senior Creativity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Hosted a 'memory lane' evening with old friend...</td>\n",
       "      <td>Gratitude</td>\n",
       "      <td>2</td>\n",
       "      <td>Friendship Adventures and Friendship Adventure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Joined a seniors' astronomy club, stargazing a...</td>\n",
       "      <td>Curiosity</td>\n",
       "      <td>0</td>\n",
       "      <td>Celestial Wonders and Celestial Wonders Senior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Attended a local jazz festival, tapping toes t...</td>\n",
       "      <td>Joy</td>\n",
       "      <td>2</td>\n",
       "      <td>Timeless Tunes and Timeless Tunes Senior Jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Started a blog sharing the wisdom gained throu...</td>\n",
       "      <td>Gratitude</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Lessons and Life Lessons Senior Blog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text       Sentiment  \\\n",
       "0   After a series of defeats, the soccer team fac...  Disappointment   \n",
       "1   In the tennis tournament, a highly anticipated...     Frustration   \n",
       "2   Facing a defeat in the championship, the boxer...      Reflection   \n",
       "3   In the midst of a cycling race, a tire blowout...        Obstacle   \n",
       "4   The gymnast's unexpected fall during a routine...        Sympathy   \n",
       "..                                                ...             ...   \n",
       "86  Organized a community painting event, turning ...             Joy   \n",
       "87  Hosted a 'memory lane' evening with old friend...       Gratitude   \n",
       "88  Joined a seniors' astronomy club, stargazing a...       Curiosity   \n",
       "89  Attended a local jazz festival, tapping toes t...             Joy   \n",
       "90  Started a blog sharing the wisdom gained throu...       Gratitude   \n",
       "\n",
       "    SimpleSent                                           Hashtags  \n",
       "0            1   Disappointment and Disappointment Soccer Defeats  \n",
       "1            1         Frustration and Frustration Tennis Setback  \n",
       "2            0            Reflection and Reflection Boxing Defeat  \n",
       "3            0          Obstacle and Obstacle Cycling Frustration  \n",
       "4            2              Sympathy and Sympathy Gymnastics Fall  \n",
       "..         ...                                                ...  \n",
       "86           2  Community Art and Community Art Senior Creativity  \n",
       "87           2  Friendship Adventures and Friendship Adventure...  \n",
       "88           0  Celestial Wonders and Celestial Wonders Senior...  \n",
       "89           2      Timeless Tunes and Timeless Tunes Senior Jazz  \n",
       "90           2          Life Lessons and Life Lessons Senior Blog  \n",
       "\n",
       "[91 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import *\n",
    "#process Hashtages\n",
    "training[\"proc_hash\"] = training.apply(lambda x: x[\"Hashtags\"][1::], axis=1)\n",
    "training[\"proc_hash\"] = training.apply(lambda x: x[\"proc_hash\"].replace(\" #\", \" and \"), axis=1)\n",
    "training[\"proc_hash\"] = training.apply(lambda x: \" and \".join(split_words(x[\"proc_hash\"].split(\" and \"))), axis=1)\n",
    "training[\"Hashtags\"] = training[\"proc_hash\"]\n",
    "training = training.drop(\"proc_hash\", axis=1)\n",
    "\n",
    "testing[\"proc_hash\"] = testing.apply(lambda x: x[\"Hashtags\"][1::], axis=1)\n",
    "testing[\"proc_hash\"] = testing.apply(lambda x: x[\"proc_hash\"].replace(\" #\", \" and \"), axis=1)\n",
    "testing[\"proc_hash\"] = testing.apply(lambda x: \" and \".join(split_words(x[\"proc_hash\"].split(\" and \"))), axis=1)\n",
    "testing[\"Hashtags\"] = testing[\"proc_hash\"]\n",
    "testing = testing.drop(\"proc_hash\", axis=1)\n",
    "\n",
    "validation[\"proc_hash\"] = validation.apply(lambda x: x[\"Hashtags\"][1::], axis=1)\n",
    "validation[\"proc_hash\"] = validation.apply(lambda x: x[\"proc_hash\"].replace(\" #\", \" and \"), axis=1)\n",
    "validation[\"proc_hash\"] = validation.apply(lambda x: \" and \".join(split_words(x[\"proc_hash\"].split(\" and \"))), axis=1)\n",
    "validation[\"Hashtags\"] = validation[\"proc_hash\"]\n",
    "validation = validation.drop(\"proc_hash\", axis=1)\n",
    "\n",
    "validation\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training[\"text_tb_pol\"] = training.apply(lambda x: TextBlob(x[\"Text\"]).polarity, axis=1)\n",
    "training[\"text_tb_sub\"] = training.apply(lambda x: TextBlob(x[\"Text\"]).subjectivity, axis=1)\n",
    "\n",
    "training[\"hash_tb_pol\"] = training.apply(lambda x: TextBlob(x[\"Hashtags\"]).polarity, axis=1)\n",
    "training[\"hash_tb_sub\"] = training.apply(lambda x: TextBlob(x[\"Hashtags\"]).subjectivity, axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "training[\"text_vader_scores\"] = training.apply(lambda x: analyzer.polarity_scores(x[\"Text\"]), axis=1)\n",
    "training[\"text_vader_neg\"] = training.apply(lambda x: x[\"text_vader_scores\"][\"neg\"], axis=1)\n",
    "training[\"text_vader_neu\"] = training.apply(lambda x: x[\"text_vader_scores\"][\"neu\"], axis=1)\n",
    "training[\"text_vader_pos\"] = training.apply(lambda x: x[\"text_vader_scores\"][\"pos\"], axis=1)\n",
    "training = training.drop([\"text_vader_scores\"], axis=1)\n",
    "\n",
    "training[\"hash_vader_scores\"] = training.apply(lambda x: analyzer.polarity_scores(x[\"Hashtags\"]), axis=1)\n",
    "training[\"hash_vader_neg\"] = training.apply(lambda x: x[\"hash_vader_scores\"][\"neg\"], axis=1)\n",
    "training[\"hash_vader_neu\"] = training.apply(lambda x: x[\"hash_vader_scores\"][\"neu\"], axis=1)\n",
    "training[\"hash_vader_pos\"] = training.apply(lambda x: x[\"hash_vader_scores\"][\"pos\"], axis=1)\n",
    "training = training.drop([\"hash_vader_scores\"], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SimpleSent</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>text_tb_pol</th>\n",
       "      <th>text_tb_sub</th>\n",
       "      <th>hash_tb_pol</th>\n",
       "      <th>hash_tb_sub</th>\n",
       "      <th>text_vader_neg</th>\n",
       "      <th>text_vader_neu</th>\n",
       "      <th>text_vader_pos</th>\n",
       "      <th>hash_vader_neg</th>\n",
       "      <th>hash_vader_neu</th>\n",
       "      <th>hash_vader_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Enjoying a beautiful day at the park!        ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Nature and Nature Park</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traffic was terrible this morning.           ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "      <td>Traffic and Traffic Morning</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Just finished an amazing workout!            ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Fitness and Fitness Workout</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excited about the upcoming weekend getaway!  ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>Travel and Travel Adventure</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trying out a new recipe for dinner tonight.  ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>Cooking and Cooking Food</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>Collaborating on a science project that receiv...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2</td>\n",
       "      <td>Science Fair Winner and Science Fair Winner Hi...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>Attending a surprise birthday party organized ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2</td>\n",
       "      <td>Surprise Celebration and Surprise Celebration ...</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>Successfully fundraising for a school charity ...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2</td>\n",
       "      <td>Community Giving and Community Giving High Sch...</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Participating in a multicultural festival, cel...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2</td>\n",
       "      <td>Cultural Celebration and Cultural Celebration ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.246667</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>Organizing a virtual talent show during challe...</td>\n",
       "      <td>Happy</td>\n",
       "      <td>2</td>\n",
       "      <td>Virtual Entertainment and Virtual Entertainmen...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>732 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Text Sentiment  SimpleSent  \\\n",
       "0     Enjoying a beautiful day at the park!        ...  Positive           2   \n",
       "1     Traffic was terrible this morning.           ...  Negative           1   \n",
       "2     Just finished an amazing workout!            ...  Positive           2   \n",
       "3     Excited about the upcoming weekend getaway!  ...  Positive           2   \n",
       "4     Trying out a new recipe for dinner tonight.  ...   Neutral           0   \n",
       "..                                                 ...       ...         ...   \n",
       "727  Collaborating on a science project that receiv...     Happy           2   \n",
       "728  Attending a surprise birthday party organized ...     Happy           2   \n",
       "729  Successfully fundraising for a school charity ...     Happy           2   \n",
       "730  Participating in a multicultural festival, cel...     Happy           2   \n",
       "731  Organizing a virtual talent show during challe...     Happy           2   \n",
       "\n",
       "                                              Hashtags  text_tb_pol  \\\n",
       "0                               Nature and Nature Park     0.750000   \n",
       "1                          Traffic and Traffic Morning    -1.000000   \n",
       "2                          Fitness and Fitness Workout     0.750000   \n",
       "3                          Travel and Travel Adventure     0.468750   \n",
       "4                             Cooking and Cooking Food     0.136364   \n",
       "..                                                 ...          ...   \n",
       "727  Science Fair Winner and Science Fair Winner Hi...     0.875000   \n",
       "728  Surprise Celebration and Surprise Celebration ...     0.687500   \n",
       "729  Community Giving and Community Giving High Sch...     0.516667   \n",
       "730  Cultural Celebration and Cultural Celebration ...     1.000000   \n",
       "731  Virtual Entertainment and Virtual Entertainmen...     0.625000   \n",
       "\n",
       "     text_tb_sub  hash_tb_pol  hash_tb_sub  text_vader_neg  text_vader_neu  \\\n",
       "0       0.800000         0.00     0.000000           0.000           0.397   \n",
       "1       1.000000         0.00     0.000000           0.437           0.563   \n",
       "2       0.900000         0.00     0.000000           0.000           0.494   \n",
       "3       0.750000         0.00     0.000000           0.000           0.650   \n",
       "4       0.454545         0.00     0.000000           0.000           1.000   \n",
       "..           ...          ...          ...             ...             ...   \n",
       "727     0.900000         0.52     0.780000           0.000           0.599   \n",
       "728     0.600000         0.16     0.540000           0.000           0.327   \n",
       "729     0.383333         0.16     0.540000           0.000           0.468   \n",
       "730     1.000000         0.12     0.246667           0.000           0.479   \n",
       "731     1.000000         0.16     0.540000           0.000           0.562   \n",
       "\n",
       "     text_vader_pos  hash_vader_neg  hash_vader_neu  hash_vader_pos  \n",
       "0             0.603             0.0           1.000           0.000  \n",
       "1             0.000             0.0           1.000           0.000  \n",
       "2             0.506             0.0           0.323           0.677  \n",
       "3             0.350             0.0           0.566           0.434  \n",
       "4             0.000             0.0           1.000           0.000  \n",
       "..              ...             ...             ...             ...  \n",
       "727           0.401             0.0           0.330           0.670  \n",
       "728           0.673             0.0           0.413           0.587  \n",
       "729           0.532             0.0           0.556           0.444  \n",
       "730           0.521             0.0           1.000           0.000  \n",
       "731           0.438             0.0           0.360           0.640  \n",
       "\n",
       "[732 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8333636363636363,\n",
       " 0.8334545454545453,\n",
       " 0.8334545454545453,\n",
       " 0.8334545454545453]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Begin logistic regression\n",
    "'''\n",
    "'''\n",
    "solver          penalty\n",
    "lbfgs           l2\n",
    "newton-cg       l1, l2\n",
    "sag             l2\n",
    "saga            elasticnet, l1, l2\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classes = training[\"SimpleSent\"].to_numpy()\n",
    "#features = training[[\"text_tb_pol\", \"text_tb_sub\", \"hash_tb_pol\", \"hash_tb_sub\", \"text_vader_neg\", \"text_vader_neu\", \"text_vader_pos\", \"hash_vader_neg\", \"hash_vader_neu\", \"hash_vader_pos\"]].to_numpy()\n",
    "\n",
    "'''text only'''\n",
    "features = training[[\"text_tb_pol\", \"text_vader_pos\", \"text_vader_neg\", \"text_vader_neu\"]].to_numpy()\n",
    "\n",
    "#validate_features = testing[[\"text_tb_pol\", \"text_tb_sub\", \"hash_tb_pol\", \"hash_tb_sub\", \"text_vader_neg\", \"text_vader_neu\", \"text_vader_pos\", \"hash_vader_neg\", \"hash_vader_neu\", \"hash_vader_pos\"]].to_numpy()\n",
    "\n",
    "\n",
    "#for i in range(100):\n",
    "\n",
    "averages = []\n",
    "\n",
    "total = 0\n",
    "length = 0\n",
    "\n",
    "#lbgfs\n",
    "for i in range(100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=i)\n",
    "    lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "    lbgfs.fit(x_train, y_train)\n",
    "    results = lbgfs.predict(x_test)\n",
    "    actual = y_test\n",
    "    total += (len([i for i in zip(results, actual) if i[0] == i[1]]) / len(actual))\n",
    "    length += 1\n",
    "averages.append(total / length)\n",
    "\n",
    "''''''\n",
    "total = 0\n",
    "length = 0\n",
    "#newton-cg\n",
    "for i in range(100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=i)\n",
    "    newton_cg = LogisticRegression(penalty=\"l2\", solver=\"newton-cg\")\n",
    "    newton_cg.fit(x_train, y_train)\n",
    "    results = newton_cg.predict(x_test)\n",
    "    actual = y_test\n",
    "    total += (len([i for i in zip(results, actual) if i[0] == i[1]]) / len(actual))\n",
    "    length += 1\n",
    "averages.append(total / length)\n",
    "\n",
    "total = 0\n",
    "length = 0\n",
    "#sag\n",
    "for i in range(100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=i)\n",
    "    sag = LogisticRegression(penalty=\"l2\", solver=\"sag\", random_state=i)\n",
    "    sag.fit(x_train, y_train)\n",
    "    results = sag.predict(x_test)\n",
    "    actual = y_test\n",
    "    total += (len([i for i in zip(results, actual) if i[0] == i[1]]) / len(actual))\n",
    "    length += 1\n",
    "averages.append(total / length)\n",
    "\n",
    "total = 0\n",
    "length = 0\n",
    "#saga\n",
    "for i in range(100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=i)\n",
    "    saga = LogisticRegression(penalty=\"l2\", solver=\"saga\", random_state=i)\n",
    "    saga.fit(x_train, y_train)\n",
    "    results = saga.predict(x_test)\n",
    "    actual = y_test\n",
    "    total += (len([i for i in zip(results, actual) if i[0] == i[1]]) / len(actual))\n",
    "    length += 1\n",
    "averages.append(total / length)\n",
    "\n",
    "\n",
    "averages\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  1.83, 10.26],\n",
       "       [ 0.  , 24.87,  4.72],\n",
       "       [ 0.  ,  1.52, 66.8 ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrices = []\n",
    "for i in range(100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=i)\n",
    "    lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "    lbgfs.fit(x_train, y_train)\n",
    "    results = lbgfs.predict(x_test)\n",
    "    actual = y_test\n",
    "    matrices.append(confusion_matrix(y_test, results))\n",
    "\n",
    "mean_matrix = np.mean(np.array(matrices), axis=0)\n",
    "mean_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, classes, test_size=0.15, random_state=0)\n",
    "lbgfs = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\")\n",
    "lbgfs.fit(x_train, y_train)\n",
    "with open(\"TWEETS_LOG.pkl\", \"wb\") as file:\n",
    "    pickle.dump(lbgfs, file)\n",
    "#results = lbgfs.predict(x_test)\n",
    "#actual = y_test\n",
    "#matrices.append(confusion_matrix(y_test, results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
